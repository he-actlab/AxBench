FANN_FLO_2.1
num_layers=3
learning_rate=0.400000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=7 9 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (9, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -2.08191852569580078125e+01) (1, -2.15391135215759277344e+00) (2, 2.78336238116025924683e-02) (3, 7.88085508346557617188e+00) (4, 7.34590625762939453125e+00) (5, 3.78704577684402465820e-01) (6, 6.46348905563354492188e+00) (0, -1.21181559562683105469e+00) (1, 9.97437059879302978516e-01) (2, -1.10876452922821044922e+00) (3, -1.67581045627593994141e+00) (4, -5.88166284561157226562e+00) (5, 6.73549175262451171875e+00) (6, 2.11297392845153808594e+00) (0, -3.88455247879028320312e+00) (1, -3.86167883872985839844e+00) (2, -3.86660861968994140625e+00) (3, 3.45932817459106445312e+00) (4, 3.34507465362548828125e-01) (5, 8.57434368133544921875e+00) (6, -2.93576288223266601562e+00) (0, 2.33332467079162597656e+00) (1, 9.27013337612152099609e-01) (2, 3.02007865905761718750e+00) (3, 2.04536247253417968750e+00) (4, 5.63719463348388671875e+00) (5, -3.78333830833435058594e+00) (6, -5.56889247894287109375e+00) (0, 1.07794561386108398438e+01) (1, 1.20413768291473388672e+00) (2, -1.99934959411621093750e+00) (3, -5.62419652938842773438e+00) (4, -5.24888372421264648438e+00) (5, 4.11052989959716796875e+00) (6, 2.06611824035644531250e+00) (0, -1.00478827953338623047e+00) (1, -1.71132016181945800781e+00) (2, -3.11652326583862304688e+00) (3, 2.37456107139587402344e+00) (4, -6.35775184631347656250e+00) (5, 1.11128339767456054688e+01) (6, -3.91430497169494628906e+00) (0, 7.53734171390533447266e-01) (1, 2.65143036842346191406e-01) (2, -1.10723257064819335938e+01) (3, -3.03981453180313110352e-01) (4, 7.34252274036407470703e-01) (5, 5.85665655136108398438e+00) (6, 4.56449413299560546875e+00) (0, -2.41718697547912597656e+00) (1, -4.08930683135986328125e+00) (2, -5.66545343399047851562e+00) (3, 1.22158384323120117188e+00) (4, 2.68106245994567871094e+00) (5, 9.87400054931640625000e-01) (6, -1.95898011326789855957e-01) (7, -9.47968542575836181641e-01) (8, -1.12915587425231933594e+00) (9, 1.32510483264923095703e+00) (10, 1.69103932380676269531e+00) (11, -5.89290320873260498047e-01) (12, 1.99600231647491455078e+00) (13, -4.15551781654357910156e-01) (14, 2.60897183418273925781e+00) (15, 1.86196509748697280884e-02) 
