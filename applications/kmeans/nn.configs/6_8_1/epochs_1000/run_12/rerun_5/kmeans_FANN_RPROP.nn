FANN_FLO_2.1
num_layers=3
learning_rate=0.400000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=7 9 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (9, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -7.87177741527557373047e-01) (1, -1.22587406635284423828e+00) (2, -3.32475113868713378906e+00) (3, -4.14553546905517578125e+00) (4, -3.53052234649658203125e+00) (5, 1.76858978271484375000e+01) (6, 1.29850104451179504395e-01) (0, 2.12670087814331054688e+00) (1, 1.05534350872039794922e+00) (2, -7.66528081893920898438e+00) (3, -3.31071376800537109375e+00) (4, 3.75898808240890502930e-01) (5, 2.92662549018859863281e+00) (6, 4.79266023635864257812e+00) (0, 1.80614638328552246094e+00) (1, -9.64864969253540039062e-01) (2, 1.85346215963363647461e-01) (3, 1.55962347984313964844e+00) (4, 1.53808581829071044922e+00) (5, -3.00137132406234741211e-01) (6, -1.53275564312934875488e-01) (0, -5.69466257095336914062e+00) (1, -1.64375817775726318359e+00) (2, -2.67322063446044921875e+00) (3, 1.40782105922698974609e+00) (4, 2.50145196914672851562e+00) (5, 1.75689983367919921875e+00) (6, -1.52652335166931152344e+00) (0, 4.95647907257080078125e-01) (1, -1.53465712070465087891e+00) (2, -4.03485202789306640625e+00) (3, 2.80689883232116699219e+00) (4, -2.88361590355634689331e-02) (5, 9.10446739196777343750e+00) (6, -5.23200035095214843750e+00) (0, 2.16494536399841308594e+00) (1, -1.25117719173431396484e+00) (2, 1.84680938720703125000e-01) (3, 1.48518002033233642578e+00) (4, 1.76989459991455078125e+00) (5, -2.01649330556392669678e-02) (6, -1.63509413599967956543e-01) (0, 6.77327108383178710938e+00) (1, 1.96771442890167236328e+00) (2, -1.91877529025077819824e-01) (3, -2.92064428329467773438e+00) (4, -2.45221424102783203125e+00) (5, -8.29984426498413085938e-01) (6, 2.71484708786010742188e+00) (0, -3.10559225082397460938e+00) (1, -1.99921584129333496094e+00) (2, -2.89015603065490722656e+00) (3, 1.96716856956481933594e+00) (4, 7.67453730106353759766e-01) (5, 1.13430583477020263672e+00) (6, -1.63569915294647216797e+00) (7, -2.42391347885131835938e+00) (8, -9.71330583095550537109e-01) (9, 5.07238507270812988281e-01) (10, 2.75437378883361816406e+00) (11, 2.67816424369812011719e+00) (12, 1.04197621345520019531e+00) (13, -4.03365343809127807617e-01) (14, 1.38374555110931396484e+00) (15, -2.72945314645767211914e-01) 
