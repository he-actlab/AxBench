FANN_FLO_2.1
num_layers=3
learning_rate=0.400000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=7 9 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (9, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 2.77226656675338745117e-01) (1, -1.24517641961574554443e-01) (2, -4.46654856204986572266e-01) (3, 5.42546585202217102051e-02) (4, -1.45730063319206237793e-01) (5, -6.00796163082122802734e-01) (6, 2.10057452321052551270e-01) (0, 1.31534671783447265625e+00) (1, -3.78007888793945312500e+00) (2, -4.93617677688598632812e+00) (3, 1.74246251583099365234e+00) (4, -2.38020157814025878906e+00) (5, 8.79032897949218750000e+00) (6, -2.08258795738220214844e+00) (0, 7.92640209197998046875e-01) (1, 3.33968460559844970703e-01) (2, -3.76955628395080566406e+00) (3, -6.75119590759277343750e+00) (4, 1.12659060955047607422e+00) (5, 6.83544349670410156250e+00) (6, 1.40979230403900146484e+00) (0, -2.39340734481811523438e+00) (1, -4.42331457138061523438e+00) (2, -3.09652686119079589844e+00) (3, 1.76677286624908447266e+00) (4, -6.02348327636718750000e-01) (5, 8.86119365692138671875e+00) (6, -1.18976986408233642578e+00) (0, -7.96368026733398437500e+00) (1, -7.42381453514099121094e-01) (2, 2.09327554702758789062e+00) (3, 4.10594844818115234375e+00) (4, 1.27796478271484375000e+01) (5, -1.33986043930053710938e+01) (6, -1.82984268665313720703e+00) (0, 8.43644046783447265625e+00) (1, -3.27409893274307250977e-01) (2, -1.61624836921691894531e+00) (3, -1.93181552886962890625e+01) (4, 6.60202026367187500000e-01) (5, 1.69469118118286132812e+00) (6, -3.87629382312297821045e-02) (0, -4.09972810745239257812e+00) (1, -3.30535173416137695312e+00) (2, -9.94303703308105468750e-01) (3, 1.57796204090118408203e+00) (4, 1.21806991100311279297e+00) (5, -6.91936194896697998047e-01) (6, -8.90147924423217773438e-01) (0, -1.51285994052886962891e+00) (1, -3.04246664047241210938e+00) (2, -3.93938851356506347656e+00) (3, 9.16197001934051513672e-01) (4, -1.52231112122535705566e-01) (5, 5.50163364410400390625e+00) (6, -2.68527960777282714844e+00) (7, -2.11146759986877441406e+00) (8, 9.80147123336791992188e-01) (9, -1.84069216251373291016e+00) (10, 1.01049435138702392578e+00) (11, 1.73966634273529052734e+00) (12, 4.06507635116577148438e+00) (13, 3.99565720558166503906e+00) (14, 2.61583161354064941406e+00) (15, -3.81264209747314453125e-01) 
