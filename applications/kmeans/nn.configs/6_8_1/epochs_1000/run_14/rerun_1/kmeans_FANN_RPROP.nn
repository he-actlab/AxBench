FANN_FLO_2.1
num_layers=3
learning_rate=0.400000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=7 9 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (9, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -8.68861770629882812500e+00) (1, -2.18565344810485839844e+00) (2, 8.02231979370117187500e+00) (3, 1.69584786891937255859e+00) (4, 2.26617470383644104004e-01) (5, -2.57498598098754882812e+00) (6, -8.15412938594818115234e-01) (0, -4.07666444778442382812e+00) (1, -2.29823327064514160156e+00) (2, -8.62200915813446044922e-01) (3, 4.62663698196411132812e+00) (4, -9.21727418899536132812e-01) (5, 2.66048741340637207031e+00) (6, -3.73186111450195312500e+00) (0, -1.80973589420318603516e+00) (1, -1.27147406339645385742e-01) (2, 3.42120361328125000000e+00) (3, 7.40832281112670898438e+00) (4, 3.04217171669006347656e+00) (5, -1.48394021987915039062e+01) (6, -6.19168579578399658203e-01) (0, -2.60521769523620605469e+00) (1, -3.23561787605285644531e+00) (2, -3.58308696746826171875e+00) (3, 1.02413010597229003906e+00) (4, 1.67502820491790771484e+00) (5, 2.34062552452087402344e+00) (6, -1.86842381954193115234e+00) (0, 2.70314574241638183594e+00) (1, -1.25093841552734375000e+00) (2, -4.74179840087890625000e+00) (3, 2.58492207527160644531e+00) (4, -2.23145532608032226562e+00) (5, 9.36760711669921875000e+00) (6, -5.03987121582031250000e+00) (0, -1.40408277511596679688e+00) (1, -2.94698429107666015625e+00) (2, -3.20597720146179199219e+00) (3, 2.28752303123474121094e+00) (4, 2.01818752288818359375e+00) (5, 2.61572813987731933594e+00) (6, -4.01926755905151367188e+00) (0, -4.07052803039550781250e+00) (1, -3.80109095573425292969e+00) (2, -6.23014545440673828125e+00) (3, 2.30047082901000976562e+00) (4, 1.26839768886566162109e+00) (5, 3.94294524192810058594e+00) (6, 6.25815153121948242188e-01) (0, -6.52081632614135742188e+00) (1, -3.00951147079467773438e+00) (2, -1.92548227310180664062e+00) (3, 1.49055790901184082031e+00) (4, 1.83357274532318115234e+00) (5, 2.48508423566818237305e-01) (6, -1.14364433288574218750e+00) (7, 9.26570832729339599609e-01) (8, 2.48211190104484558105e-01) (9, 2.60683202743530273438e+00) (10, 7.81442701816558837891e-01) (11, 2.34513783454895019531e+00) (12, 2.34059309959411621094e+00) (13, 5.34434139728546142578e-01) (14, 2.12231969833374023438e+00) (15, -2.55408287048339843750e+00) 
