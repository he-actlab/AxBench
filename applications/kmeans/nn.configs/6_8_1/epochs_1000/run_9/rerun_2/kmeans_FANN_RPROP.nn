FANN_FLO_2.1
num_layers=3
learning_rate=0.400000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=7 9 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (9, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -4.56608390808105468750e+00) (1, -1.91043663024902343750e+00) (2, -3.98189520835876464844e+00) (3, 5.19556903839111328125e+00) (4, 1.81760478019714355469e+00) (5, 4.38085985183715820312e+00) (6, -2.05082845687866210938e+00) (0, -2.61791157722473144531e+00) (1, -2.91872000694274902344e+00) (2, -5.92764914035797119141e-01) (3, 2.70966815948486328125e+00) (4, -2.23873186111450195312e+00) (5, 1.27593784332275390625e+01) (6, -6.20727205276489257812e+00) (0, 1.90831947326660156250e+00) (1, 1.16666233539581298828e+00) (2, -2.40170311927795410156e+00) (3, -1.44408440589904785156e+00) (4, -3.51963567733764648438e+00) (5, 6.58801603317260742188e+00) (6, 2.20018312335014343262e-01) (0, 1.78388178348541259766e+00) (1, 1.31775295734405517578e+00) (2, -2.31717586517333984375e+00) (3, -1.46382737159729003906e+00) (4, -3.26651048660278320312e+00) (5, 5.77470493316650390625e+00) (6, 2.93129742145538330078e-01) (0, 1.99660897254943847656e+00) (1, 1.97209608554840087891e+00) (2, -6.26653432846069335938e+00) (3, 1.53074193000793457031e+00) (4, -2.55538964271545410156e+00) (5, 9.38688755035400390625e+00) (6, 4.07979071140289306641e-01) (0, -8.51799774169921875000e+00) (1, -1.73985803127288818359e+00) (2, -9.13000464439392089844e-01) (3, 1.78596615791320800781e+00) (4, 2.51516532897949218750e+00) (5, -1.61098396778106689453e+00) (6, -1.55333137512207031250e+00) (0, 2.44722437858581542969e+00) (1, -1.10726130008697509766e+00) (2, -6.14019453525543212891e-01) (3, 1.21981430053710937500e+00) (4, 2.52627563476562500000e+00) (5, -2.95101881027221679688e-01) (6, 3.80984514951705932617e-01) (0, -3.20443606376647949219e+00) (1, -3.52064990997314453125e+00) (2, -3.02649712562561035156e+00) (3, 2.89761209487915039062e+00) (4, 1.69871973991394042969e+00) (5, 6.18044185638427734375e+00) (6, -6.60364770889282226562e+00) (7, 7.77819216251373291016e-01) (8, 1.93486273288726806641e+00) (9, -9.39350008964538574219e-01) (10, -1.15303766727447509766e+00) (11, -1.48918640613555908203e+00) (12, 3.98445725440979003906e+00) (13, 9.06506657600402832031e-01) (14, 5.16985797882080078125e+00) (15, 3.65742594003677368164e-02) 
