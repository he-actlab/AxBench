FANN_FLO_2.1
num_layers=3
learning_rate=0.400000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=7 9 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (9, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -5.92327594757080078125e+00) (1, -2.23916769027709960938e+00) (2, -2.09119606018066406250e+00) (3, 1.31509077548980712891e+00) (4, 7.32627689838409423828e-01) (5, 4.37483739852905273438e+00) (6, -1.11181735992431640625e+00) (0, -1.77681505680084228516e+00) (1, 1.12938427925109863281e+00) (2, -4.69143800437450408936e-02) (3, -1.56662809848785400391e+00) (4, -1.52321839332580566406e+00) (5, -9.79010283946990966797e-01) (6, 1.58380818367004394531e+00) (0, -1.88933506608009338379e-01) (1, 4.77556824684143066406e-01) (2, -2.28264904022216796875e+00) (3, -1.85998201370239257812e+00) (4, -1.71833741664886474609e+00) (5, -4.80260789394378662109e-01) (6, 2.02526235580444335938e+00) (0, 7.36026167869567871094e-01) (1, 9.69274401664733886719e-01) (2, -5.03333759307861328125e+00) (3, -6.47147238254547119141e-01) (4, -9.94893169403076171875e+00) (5, 1.51543674468994140625e+01) (6, 2.44689893722534179688e+00) (0, -1.01942396163940429688e+00) (1, -1.08844184875488281250e+00) (2, 2.06903624534606933594e+00) (3, 5.85666596889495849609e-01) (4, 5.65786480903625488281e-01) (5, 2.17711782455444335938e+00) (6, -8.17140698432922363281e-01) (0, -1.55866518616676330566e-01) (1, -1.07831633090972900391e+00) (2, -3.30187463760375976562e+00) (3, 1.56652796268463134766e+00) (4, -2.29135775566101074219e+00) (5, 4.57013607025146484375e+00) (6, -1.01015366613864898682e-01) (0, 6.61259114742279052734e-01) (1, -1.07735037803649902344e+00) (2, 1.12439131736755371094e+00) (3, 2.73402184247970581055e-01) (4, 1.54817175865173339844e+00) (5, 1.39162576198577880859e+00) (6, 8.96629542112350463867e-02) (0, 1.72245502471923828125e+00) (1, 2.47841501235961914062e+00) (2, 1.90414314270019531250e+01) (3, -1.62896251678466796875e+00) (4, -8.18240547180175781250e+00) (5, -1.61858987808227539062e+00) (6, 1.67477607727050781250e+00) (7, 2.84282636642456054688e+00) (8, -1.28700149059295654297e+00) (9, -3.39296460151672363281e+00) (10, -2.49248528480529785156e+00) (11, 1.45209908485412597656e+00) (12, 1.76474201679229736328e+00) (13, 9.07100260257720947266e-01) (14, -1.01139318943023681641e+00) (15, 8.75000804662704467773e-02) 
