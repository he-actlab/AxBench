FANN_FLO_2.1
num_layers=3
learning_rate=0.400000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=7 9 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (9, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 8.76150131225585937500e+00) (1, -9.87513732910156250000e+00) (2, 5.13831901550292968750e+00) (3, -2.93718481063842773438e+00) (4, -3.93464893102645874023e-01) (5, -3.37823247909545898438e+00) (6, 4.65868139266967773438e+00) (0, 2.67255783081054687500e+00) (1, 1.98609992861747741699e-01) (2, 3.30966072082519531250e+01) (3, -4.39175605773925781250e+00) (4, -1.23631887435913085938e+01) (5, 2.11726427078247070312e+00) (6, 2.98641026020050048828e-01) (0, -5.88046848773956298828e-01) (1, -1.02619142532348632812e+01) (2, 2.54968261718750000000e+01) (3, 4.37345933914184570312e+00) (4, -1.53546142578125000000e+00) (5, 2.35156774520874023438e+00) (6, 4.78409826755523681641e-01) (0, -8.06964457035064697266e-01) (1, -9.55016851425170898438e-01) (2, -8.45063972473144531250e+00) (3, -3.07584404945373535156e+00) (4, -7.33756959438323974609e-01) (5, 7.89987862110137939453e-01) (6, 1.38611793518066406250e+00) (0, 6.38567447662353515625e+00) (1, -9.05880546569824218750e+00) (2, -1.79501838684082031250e+01) (3, -2.54568004608154296875e+00) (4, -1.89810180664062500000e+00) (5, -1.55157220363616943359e+00) (6, 5.15375185012817382812e+00) (0, -7.92839407920837402344e-01) (1, -9.07790660858154296875e-01) (2, -7.87197589874267578125e+00) (3, -7.57031106948852539062e+00) (4, 8.39568316936492919922e-01) (5, 1.94067561626434326172e+00) (6, 4.94130402803421020508e-01) (0, -1.24673616886138916016e+00) (1, -4.68114435672760009766e-01) (2, 1.19445502758026123047e-01) (3, -3.51864981651306152344e+00) (4, -1.50889182090759277344e+00) (5, 1.98515564203262329102e-01) (6, 5.28865456581115722656e-01) (0, -1.59878568649291992188e+01) (1, 1.43339796066284179688e+01) (2, -3.63461995124816894531e+00) (3, -3.03917789459228515625e+00) (4, -1.04795205593109130859e+00) (5, 4.50112819671630859375e+00) (6, 2.17028927803039550781e+00) (7, -4.56823635101318359375e+00) (8, -6.24808132648468017578e-01) (9, -5.61339497566223144531e-01) (10, -5.59195339679718017578e-01) (11, -6.40095055103302001953e-01) (12, -4.75135654211044311523e-01) (13, 6.54677581787109375000e+00) (14, -4.37660312652587890625e+00) (15, 3.29225873947143554688e+00) 
