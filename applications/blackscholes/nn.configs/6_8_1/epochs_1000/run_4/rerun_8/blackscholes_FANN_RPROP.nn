FANN_FLO_2.1
num_layers=3
learning_rate=0.400000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=7 9 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (9, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 3.09366226196289062500e-01) (1, -8.29666042327880859375e+00) (2, -2.75009775161743164062e+00) (3, -1.60446758270263671875e+01) (4, -4.40324878692626953125e+00) (5, -3.96566182374954223633e-01) (6, 1.60158271789550781250e+01) (0, 1.28730070590972900391e+00) (1, -1.40990095138549804688e+01) (2, -8.98855209350585937500e+00) (3, 8.61496388912200927734e-01) (4, 6.71583563089370727539e-02) (5, -3.80629086494445800781e+00) (6, 1.53986444473266601562e+01) (0, -1.77536201477050781250e+00) (1, -1.15025053024291992188e+01) (2, -4.33466196060180664062e+00) (3, 2.38318061828613281250e+00) (4, -3.23308892548084259033e-02) (5, -2.45533084869384765625e+00) (6, 1.43727912902832031250e+01) (0, -1.85998058319091796875e+01) (1, 1.81675205230712890625e+01) (2, -1.38605947494506835938e+01) (3, 9.16843473911285400391e-01) (4, 1.45960152149200439453e-01) (5, 5.44005632400512695312e+00) (6, 1.26909542083740234375e+00) (0, 3.77989411354064941406e+00) (1, -1.37017834186553955078e+00) (2, -1.81661403179168701172e+00) (3, -2.48689770698547363281e+00) (4, 1.56459033489227294922e-01) (5, -4.39395129680633544922e-01) (6, -9.79753673076629638672e-01) (0, 7.99360847473144531250e+00) (1, -7.92985868453979492188e+00) (2, 3.67908744812011718750e+01) (3, 7.01679527759552001953e-01) (4, -3.84624958038330078125e+00) (5, 1.31748378276824951172e+00) (6, -1.38722288608551025391e+00) (0, -5.74743938446044921875e+00) (1, -1.08071956634521484375e+01) (2, 9.42006492614746093750e+00) (3, 2.67637205123901367188e+00) (4, -5.67273569107055664062e+00) (5, -8.83867931365966796875e+00) (6, 1.49752082824707031250e+01) (0, 4.11726617813110351562e+00) (1, -1.02339394390583038330e-01) (2, -4.65092039108276367188e+00) (3, -7.99551916122436523438e+00) (4, 3.21297693252563476562e+00) (5, -9.65891361236572265625e-01) (6, -2.64532494544982910156e+00) (7, -6.35939240455627441406e-01) (8, -9.80460047721862792969e-01) (9, -3.72069478034973144531e-01) (10, -2.81759977340698242188e+00) (11, -2.78613448143005371094e-01) (12, -1.16119551658630371094e+00) (13, -1.05702245235443115234e+00) (14, -1.36037051677703857422e+00) (15, 2.55018806457519531250e+00) 
