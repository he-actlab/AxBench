FANN_FLO_2.1
num_layers=3
learning_rate=0.400000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=7 9 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (9, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -7.59182691574096679688e-01) (1, -9.18439686298370361328e-01) (2, 2.62179684638977050781e+00) (3, -3.92840892076492309570e-01) (4, -9.25112485885620117188e-01) (5, 7.09436988830566406250e+00) (6, -3.57958412170410156250e+00) (0, -5.02868950366973876953e-01) (1, 3.70027244091033935547e-01) (2, 3.41958389282226562500e+01) (3, -3.47848081588745117188e+00) (4, -1.79615652561187744141e+00) (5, 3.29394340515136718750e+00) (6, -1.83805119991302490234e+00) (0, -1.42252612113952636719e+00) (1, -1.11781418323516845703e+00) (2, 2.60820293426513671875e+01) (3, -1.05854768753051757812e+01) (4, -1.81472408771514892578e+00) (5, 5.91022968292236328125e+00) (6, -1.60068881511688232422e+00) (0, -6.37249612808227539062e+00) (1, 5.02283859252929687500e+00) (2, -3.15790295600891113281e+00) (3, -1.95727467536926269531e+00) (4, -1.48592853546142578125e+00) (5, -4.49612236022949218750e+00) (6, 4.30158227682113647461e-01) (0, 5.32745218276977539062e+00) (1, -7.49272251129150390625e+00) (2, -5.46880960464477539062e+00) (3, -3.02076005935668945312e+00) (4, -1.73128139972686767578e+00) (5, 3.50590538978576660156e+00) (6, -1.71409916877746582031e+00) (0, 1.01234173774719238281e+00) (1, -4.38643074035644531250e+00) (2, 5.05376398563385009766e-01) (3, -1.07476663589477539062e+00) (4, 3.12936186790466308594e+00) (5, 6.97924196720123291016e-02) (6, -1.27376109361648559570e-01) (0, -2.20738649368286132812e+00) (1, -1.75124418735504150391e+00) (2, -6.45674037933349609375e+00) (3, -5.39311218261718750000e+00) (4, -3.14304161071777343750e+00) (5, 8.01678895950317382812e-01) (6, 3.58716845512390136719e+00) (0, 1.40650844573974609375e+00) (1, 6.02252244949340820312e-01) (2, 6.14849042892456054688e+00) (3, -1.12958641052246093750e+01) (4, -1.00274896621704101562e+01) (5, 5.33381462097167968750e+00) (6, 5.71851444244384765625e+00) (7, -4.44617211818695068359e-01) (8, -4.91307318210601806641e-01) (9, -1.04623174667358398438e+00) (10, -1.90874671936035156250e+01) (11, -1.31817502975463867188e+01) (12, -1.08573222160339355469e+00) (13, 5.95882701873779296875e+00) (14, -1.57202377915382385254e-01) (15, -4.00369286537170410156e-01) 
