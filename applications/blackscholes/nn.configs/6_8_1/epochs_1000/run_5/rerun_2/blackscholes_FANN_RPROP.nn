FANN_FLO_2.1
num_layers=3
learning_rate=0.400000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=7 9 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (9, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -1.56231451034545898438e+00) (1, -5.90347099304199218750e+00) (2, 6.51295995712280273438e+00) (3, 3.02627265453338623047e-01) (4, -8.77708911895751953125e+00) (5, 1.00794591903686523438e+01) (6, -1.72734364867210388184e-01) (0, 5.15920782089233398438e+00) (1, -1.08289661407470703125e+01) (2, 1.98961198329925537109e+00) (3, -6.42198944091796875000e+00) (4, 2.52236843109130859375e+00) (5, 3.48567390441894531250e+00) (6, -2.89056003093719482422e-01) (0, -8.71901988983154296875e+00) (1, -2.78616976737976074219e+00) (2, 2.71703128814697265625e+01) (3, -6.64055347442626953125e-01) (4, -6.36970233917236328125e+00) (5, 8.04743099212646484375e+00) (6, 1.10553417205810546875e+01) (0, 1.29082179069519042969e+00) (1, -1.43690383434295654297e+00) (2, -2.22999076843261718750e+01) (3, 5.23120927810668945312e+00) (4, -4.56319236755371093750e+00) (5, 1.54725611209869384766e+00) (6, -1.34612905979156494141e+00) (0, -1.24187097549438476562e+01) (1, 1.15638418197631835938e+01) (2, -3.33913683891296386719e+00) (3, -9.39749717712402343750e-01) (4, -8.70302796363830566406e-01) (5, -1.50000000000000000000e+03) (6, 1.86056002974510192871e-01) (0, 5.17626476287841796875e+00) (1, -2.02317520976066589355e-01) (2, -4.89025831222534179688e+00) (3, -5.61149787902832031250e+00) (4, 7.73612260818481445312e-01) (5, 2.16346740722656250000e+00) (6, -5.15752029418945312500e+00) (0, 1.19171457290649414062e+01) (1, -1.25818271636962890625e+01) (2, 1.53280897140502929688e+01) (3, 3.74898290634155273438e+00) (4, 2.88024604320526123047e-01) (5, -3.95223331451416015625e+00) (6, 3.58455634117126464844e+00) (0, 1.11360418796539306641e+00) (1, -1.95595204830169677734e-01) (2, -2.48036270141601562500e+01) (3, -5.67045783996582031250e+00) (4, -2.89016962051391601562e+00) (5, 5.63627195358276367188e+00) (6, -1.97856771945953369141e+00) (7, -1.10404634475708007812e+00) (8, -2.47050905227661132812e+00) (9, -4.76015359163284301758e-01) (10, -5.24745285511016845703e-01) (11, -6.14442443847656250000e+00) (12, -2.97735428810119628906e+00) (13, -2.35095834732055664062e+00) (14, -2.20241591334342956543e-01) (15, 1.59108674526214599609e+00) 
