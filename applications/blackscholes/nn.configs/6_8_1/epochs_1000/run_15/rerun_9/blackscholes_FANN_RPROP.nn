FANN_FLO_2.1
num_layers=3
learning_rate=0.400000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=7 9 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (9, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 1.63379740715026855469e+00) (1, 2.60822510719299316406e+00) (2, 4.30053758621215820312e+00) (3, -7.75564908981323242188e+00) (4, -3.60406446456909179688e+00) (5, 2.37718367576599121094e+00) (6, -1.71623635292053222656e+00) (0, -5.65850353240966796875e+00) (1, 4.50523328781127929688e+00) (2, -6.97515821456909179688e+00) (3, 4.44479852914810180664e-01) (4, -1.39527726173400878906e+00) (5, -4.61591243743896484375e+00) (6, 2.21887618303298950195e-01) (0, 4.24472713470458984375e+00) (1, 4.11739397048950195312e+00) (2, 3.88726921081542968750e+01) (3, -1.47680397033691406250e+01) (4, -4.07241916656494140625e+00) (5, -5.50591325759887695312e+00) (6, 1.41561388969421386719e+00) (0, -2.38317179679870605469e+00) (1, -1.36305599212646484375e+01) (2, 4.36953639984130859375e+00) (3, -2.74293613433837890625e+00) (4, -4.09425795078277587891e-01) (5, 5.43793535232543945312e+00) (6, 3.89106720685958862305e-01) (0, -5.86644220352172851562e+00) (1, 4.58293199539184570312e+00) (2, -1.84761583805084228516e+00) (3, -2.69484591484069824219e+00) (4, 1.28247842192649841309e-01) (5, 9.53895473480224609375e+00) (6, 6.45906746387481689453e-01) (0, -2.10596036911010742188e+00) (1, 9.96439516544342041016e-01) (2, -2.08192634582519531250e+01) (3, -5.08835697174072265625e+00) (4, -5.46875953674316406250e+00) (5, 1.71117007732391357422e-01) (6, 1.39446270465850830078e+00) (0, -5.62664651870727539062e+00) (1, 4.74323225021362304688e+00) (2, -1.86724328994750976562e+00) (3, -2.33289313316345214844e+00) (4, 3.29236350953578948975e-02) (5, 9.74651145935058593750e+00) (6, 4.58657771348953247070e-01) (0, -4.71169853210449218750e+00) (1, 1.60764753818511962891e+00) (2, 9.07734680175781250000e+00) (3, -2.59335207939147949219e+00) (4, 2.85746479034423828125e+00) (5, -5.89072799682617187500e+00) (6, 5.75621366500854492188e-01) (7, -2.07301688194274902344e+00) (8, -7.87398767471313476562e+00) (9, -5.58137655258178710938e-01) (10, -9.64585685729980468750e+00) (11, -1.33934128284454345703e+00) (12, 4.83904933929443359375e+00) (13, -7.08302438259124755859e-01) (14, -1.13488030433654785156e+00) (15, 3.70950967073440551758e-01) 
