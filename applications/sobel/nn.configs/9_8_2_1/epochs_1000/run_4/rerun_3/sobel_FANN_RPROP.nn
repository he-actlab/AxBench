FANN_FLO_2.1
num_layers=4
learning_rate=0.500000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=10 9 3 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (3, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -6.25841394066810607910e-02) (1, 7.31850576400756835938e+00) (2, 1.16212725639343261719e+00) (3, -1.86484551429748535156e+00) (4, 2.34075084328651428223e-01) (5, -1.76941648125648498535e-01) (6, -1.61576873779296875000e+02) (7, -1.93336582183837890625e+00) (8, -5.19559717178344726562e+00) (9, 2.12269806861877441406e+00) (0, 1.79235428571701049805e-01) (1, 1.32749576568603515625e+01) (2, -2.90343928337097167969e+00) (3, -1.05217659473419189453e+00) (4, 2.61525273323059082031e-01) (5, -7.88545131683349609375e-01) (6, -1.00869913101196289062e+01) (7, -2.75287127494812011719e+00) (8, -4.89816284179687500000e+00) (9, 1.38252758979797363281e+00) (0, -2.37007886171340942383e-02) (1, 1.59126253128051757812e+01) (2, -1.25421118736267089844e+00) (3, -2.27742338180541992188e+00) (4, 5.40403500199317932129e-02) (5, -1.86331585049629211426e-01) (6, -1.98974571228027343750e+01) (7, -5.07423782348632812500e+00) (8, -4.72697734832763671875e+00) (9, 1.36670839786529541016e+00) (0, -7.38940715789794921875e-01) (1, -1.75460071563720703125e+01) (2, 8.12816047668457031250e+00) (3, 1.76544094085693359375e+00) (4, -1.14452338218688964844e+00) (5, 1.40049621462821960449e-01) (6, 1.04882249832153320312e+01) (7, 3.26576209068298339844e+00) (8, 5.48500585556030273438e+00) (9, -1.79135918617248535156e+00) (0, 4.35102283954620361328e-01) (1, 1.85319023132324218750e+01) (2, -5.01973247528076171875e+00) (3, -1.17263066768646240234e+00) (4, 8.47169384360313415527e-02) (5, -1.78539916872978210449e-01) (6, -1.55085744857788085938e+01) (7, -5.72181749343872070312e+00) (8, -5.19629526138305664062e+00) (9, 2.33508157730102539062e+00) (0, 9.93522524833679199219e-01) (1, 3.17657828330993652344e+00) (2, -4.97149288654327392578e-01) (3, 5.01118421554565429688e+00) (4, 5.94002628326416015625e+00) (5, -1.38713920116424560547e+00) (6, 3.58500313758850097656e+00) (7, 7.89826214313507080078e-01) (8, -6.84885635375976562500e+01) (9, 5.35536384582519531250e+00) (0, -1.22327044606208801270e-01) (1, 3.82333683967590332031e+00) (2, 9.95622217655181884766e-01) (3, -2.57813239097595214844e+00) (4, -1.13513935357332229614e-02) (5, -3.49221199750900268555e-01) (6, -6.90188598632812500000e+01) (7, -1.87882268428802490234e+00) (8, -5.12299203872680664062e+00) (9, 3.53305554389953613281e+00) (0, 2.83053405582904815674e-02) (1, 3.59191513061523437500e+01) (2, -8.18329715728759765625e+00) (3, -3.07247447967529296875e+00) (4, -1.33616486564278602600e-02) (5, 4.83357071876525878906e-01) (6, -3.98773880004882812500e+01) (7, -6.25515222549438476562e+00) (8, -4.38837432861328125000e+00) (9, 1.80705904960632324219e+00) (10, 1.50000000000000000000e+03) (11, 1.49711560058593750000e+03) (12, 1.50000000000000000000e+03) (13, -5.96363098144531250000e+02) (14, 1.50000000000000000000e+03) (15, 1.50000000000000000000e+03) (16, 1.50000000000000000000e+03) (17, 1.50000000000000000000e+03) (18, -1.50767719745635986328e+00) (10, 1.50000000000000000000e+03) (11, 1.50000000000000000000e+03) (12, 1.50000000000000000000e+03) (13, -3.44151489257812500000e+02) (14, 1.50000000000000000000e+03) (15, 1.50000000000000000000e+03) (16, 1.50000000000000000000e+03) (17, 1.50000000000000000000e+03) (18, 7.71736860275268554688e-01) (19, 1.15087437629699707031e+00) (20, 1.62222051620483398438e+00) (21, -1.91792929172515869141e+00) 
