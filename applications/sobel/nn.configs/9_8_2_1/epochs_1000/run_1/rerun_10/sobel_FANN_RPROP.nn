FANN_FLO_2.1
num_layers=4
learning_rate=0.500000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=10 9 3 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (3, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 9.13386046886444091797e-02) (1, -6.83900880813598632812e+00) (2, -3.05474519729614257812e-01) (3, 1.70979201793670654297e+00) (4, -4.79968786239624023438e-01) (5, 1.04506038129329681396e-01) (6, -1.05627858638763427734e+00) (7, 7.02904403209686279297e-01) (8, 2.13152194023132324219e+00) (9, 6.40581905841827392578e-01) (0, 2.35999301075935363770e-01) (1, 8.20584487915039062500e+00) (2, 6.19199499487876892090e-02) (3, -4.92393821477890014648e-01) (4, -3.51393985748291015625e+00) (5, -1.03441607952117919922e+00) (6, -1.19292092323303222656e+00) (7, -8.74731600284576416016e-01) (8, -7.25180292129516601562e+00) (9, -5.42350411415100097656e-01) (0, 2.40858006477355957031e+00) (1, -2.68703103065490722656e+00) (2, 3.19640666246414184570e-01) (3, -3.94720935821533203125e+00) (4, -1.89418900012969970703e+00) (5, 7.10846483707427978516e-02) (6, 2.91639864444732666016e-01) (7, 2.25046142935752868652e-01) (8, -3.68264102935791015625e+00) (9, 3.79985392093658447266e-01) (0, -3.89143347740173339844e-01) (1, 1.17854862213134765625e+01) (2, 1.35391846299171447754e-01) (3, -7.46425330638885498047e-01) (4, -3.76088070869445800781e+00) (5, -5.90619027614593505859e-01) (6, -2.45034933090209960938e+00) (7, 9.60701778531074523926e-02) (8, -6.76327705383300781250e+00) (9, -1.66608929634094238281e+00) (0, -2.19448227435350418091e-02) (1, 1.00043134689331054688e+01) (2, -5.10569512844085693359e-02) (3, -8.45588147640228271484e-01) (4, -5.34070920944213867188e+00) (5, -9.99553143978118896484e-01) (6, -2.41779208183288574219e+00) (7, -1.73169504851102828979e-02) (8, -6.05879545211791992188e+00) (9, -7.39404022693634033203e-01) (0, -2.48755168914794921875e+00) (1, -8.60915756225585937500e+00) (2, -1.51184864044189453125e+01) (3, -2.43639513850212097168e-01) (4, 2.24192470312118530273e-01) (5, 3.94209106445312500000e+02) (6, -2.41039490699768066406e+00) (7, -4.99615252017974853516e-01) (8, -4.38761230468750000000e+02) (9, 6.58466696739196777344e-01) (0, -4.00467738509178161621e-02) (1, 1.05917654037475585938e+01) (2, 3.48366737365722656250e-01) (3, -7.85599887371063232422e-01) (4, -1.77212333679199218750e+00) (5, -1.00795936584472656250e+00) (6, -2.95212912559509277344e+00) (7, -1.45941570401191711426e-01) (8, -5.32217931747436523438e+00) (9, -2.30756568908691406250e+00) (0, 1.87260612845420837402e-01) (1, 8.87715625762939453125e+00) (2, -6.07191741466522216797e-01) (3, -8.45076501369476318359e-01) (4, -1.73969054222106933594e+00) (5, -1.43908715248107910156e+00) (6, -1.33582568168640136719e+00) (7, -5.77635288238525390625e-01) (8, -7.11288404464721679688e+00) (9, -7.17332661151885986328e-01) (10, 9.73030185699462890625e+00) (11, -4.30394029617309570312e+00) (12, -7.28916198730468750000e+02) (13, 7.64141159057617187500e+01) (14, 2.99082641601562500000e+01) (15, 2.81488437652587890625e+01) (16, -1.56865749359130859375e+01) (17, -4.11583614349365234375e+00) (18, 4.99369144439697265625e-01) (10, -3.13095417022705078125e+01) (11, 9.03786087036132812500e+00) (12, 6.25999259948730468750e+01) (13, -9.08029842376708984375e+00) (14, -5.00843620300292968750e+00) (15, 1.07011687755584716797e+00) (16, -9.72816753387451171875e+00) (17, -7.38622045516967773438e+00) (18, 4.65245151519775390625e+00) (19, 2.04484343528747558594e+00) (20, -2.75887322425842285156e+00) (21, -1.13254165649414062500e+00) 
