FANN_FLO_2.1
num_layers=4
learning_rate=0.500000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=10 9 3 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (3, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -3.11200261116027832031e-01) (1, 8.95270156860351562500e+00) (2, 2.48196554183959960938e+00) (3, 1.58753424882888793945e-01) (4, -6.96841669082641601562e+00) (5, -3.13898533582687377930e-01) (6, -1.07362973690032958984e+00) (7, -4.64386761188507080078e-01) (8, -1.12809181213378906250e+00) (9, -2.18474078178405761719e+00) (0, 4.10574531555175781250e+00) (1, 1.37387495040893554688e+01) (2, 5.75516819953918457031e-01) (3, 2.06149935722351074219e+00) (4, -2.98759603500366210938e+00) (5, -3.27824974060058593750e+00) (6, -4.20569241046905517578e-01) (7, -3.84924650192260742188e-01) (8, -1.55650682449340820312e+01) (9, -2.46587395668029785156e+00) (0, -3.30215110778808593750e+01) (1, 2.75827813148498535156e+00) (2, -4.51935052871704101562e+00) (3, -1.70958137512207031250e+01) (4, 1.93273794651031494141e+00) (5, -3.07977604866027832031e+00) (6, -9.46834564208984375000e+00) (7, -8.15360069274902343750e-01) (8, -1.32455932617187500000e+03) (9, -5.07167160511016845703e-01) (0, -3.50129318237304687500e+01) (1, 8.70586395263671875000e+00) (2, 3.37922263145446777344e+00) (3, -1.05498068034648895264e-01) (4, 2.80103951692581176758e-01) (5, -5.57096004486083984375e-01) (6, -4.45178180932998657227e-01) (7, 1.18563205003738403320e-01) (8, 1.17796220779418945312e+01) (9, -1.41270607709884643555e-02) (0, -3.05365696549415588379e-02) (1, 8.89818572998046875000e+00) (2, 1.68241167068481445312e+00) (3, 3.88218474388122558594e+00) (4, -8.59513473510742187500e+00) (5, -9.74305876297876238823e-05) (6, -1.09500277042388916016e+00) (7, -5.47983944416046142578e-01) (8, -7.29691171646118164062e+00) (9, -1.10181200504302978516e+00) (0, 1.36366786956787109375e+01) (1, 5.84262990951538085938e+00) (2, -2.54754543304443359375e-01) (3, 1.75620353221893310547e+00) (4, -2.20018339157104492188e+00) (5, -5.41843700408935546875e+00) (6, -7.71185383200645446777e-02) (7, -7.98936009407043457031e-01) (8, -1.19075632095336914062e+01) (9, -1.95648622512817382812e+00) (0, -3.96977806091308593750e+01) (1, 1.92876267433166503906e+00) (2, -4.41525554656982421875e+00) (3, -4.15816040039062500000e+01) (4, 7.50362205505371093750e+00) (5, -3.28120350837707519531e+00) (6, -1.34679470062255859375e+01) (7, -1.95875778794288635254e-01) (8, -1.32455932617187500000e+03) (9, -2.62725502252578735352e-01) (0, -1.17320990562438964844e+00) (1, 1.06758909225463867188e+01) (2, 1.10466873645782470703e+00) (3, -1.63278889656066894531e+00) (4, -5.11873674392700195312e+00) (5, 6.98066592216491699219e-01) (6, -1.89556777477264404297e-01) (7, -5.67450404167175292969e-01) (8, -7.00166511535644531250e+00) (9, -3.04248929023742675781e-01) (10, 3.81134533882141113281e+00) (11, 7.79597520828247070312e+00) (12, -1.50000000000000000000e+03) (13, 1.11177921295166015625e+01) (14, 2.50103020668029785156e+00) (15, 2.26988482475280761719e+00) (16, -1.50000000000000000000e+03) (17, -7.07155823707580566406e-01) (18, -6.90111160278320312500e+00) (10, 6.56267404556274414062e+00) (11, -1.94376096129417419434e-01) (12, -1.50000000000000000000e+03) (13, 2.24324264526367187500e+01) (14, 9.11204576492309570312e-01) (15, 2.45926074981689453125e+01) (16, -1.50000000000000000000e+03) (17, -1.71128330230712890625e+01) (18, -7.49668741226196289062e+00) (19, 1.89155244827270507812e+00) (20, 1.03584098815917968750e+00) (21, -2.04066419601440429688e+00) 
