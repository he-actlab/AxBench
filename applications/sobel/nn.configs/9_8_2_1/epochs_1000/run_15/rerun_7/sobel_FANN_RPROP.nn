FANN_FLO_2.1
num_layers=4
learning_rate=0.500000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=10 9 3 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (3, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 3.00569987297058105469e+00) (1, -2.57166790962219238281e+00) (2, -4.63880151510238647461e-01) (3, 3.17196130752563476562e+00) (4, 5.20818614959716796875e+00) (5, -3.11048793792724609375e+00) (6, 1.51193561553955078125e+01) (7, -3.48637962341308593750e+00) (8, -5.97966766357421875000e+00) (9, -5.29985237121582031250e+00) (0, 5.45685815811157226562e+00) (1, -1.18903505802154541016e+00) (2, -1.64519834518432617188e+00) (3, 9.82863616943359375000e+00) (4, -6.63334369659423828125e-01) (5, -2.64502549171447753906e+00) (6, 5.15669107437133789062e+00) (7, 1.02025651931762695312e+00) (8, -9.75111675262451171875e+00) (9, 1.61368155479431152344e+00) (0, -1.35212993621826171875e+01) (1, 4.11086130142211914062e+00) (2, 2.54710340499877929688e+00) (3, -5.16980266571044921875e+00) (4, 1.33295521140098571777e-01) (5, 2.22972536087036132812e+00) (6, -6.63128137588500976562e+00) (7, -2.08509945869445800781e+00) (8, 5.46056556701660156250e+00) (9, -1.35650384426116943359e+00) (0, 3.08840427398681640625e+01) (1, 1.76703796386718750000e+01) (2, 3.05950546264648437500e+00) (3, -3.15883278846740722656e+00) (4, 9.86296272277832031250e+00) (5, 1.07106504440307617188e+01) (6, -3.18579063415527343750e+01) (7, -6.99138336181640625000e+01) (8, 5.61011195182800292969e-01) (9, -1.14339179992675781250e+01) (0, -2.14743685722351074219e+00) (1, 2.05274176597595214844e+00) (2, 2.05243563652038574219e+00) (3, -3.69097065925598144531e+00) (4, 1.34576690196990966797e+00) (5, 1.02262420654296875000e+01) (6, -1.03173885345458984375e+01) (7, -8.99686396121978759766e-01) (8, 2.37086820602416992188e+00) (9, -1.18761312961578369141e+00) (0, 2.68738317489624023438e+00) (1, -1.02046155929565429688e+00) (2, -1.86465418338775634766e+00) (3, 2.79435253143310546875e+00) (4, -1.03588414192199707031e+00) (5, -3.63547658920288085938e+00) (6, 5.70165205001831054688e+00) (7, 1.31616234779357910156e+00) (8, -1.16371803283691406250e+01) (9, 2.96299719810485839844e+00) (0, 8.48109008789062500000e+02) (1, 1.11578698730468750000e+03) (2, 1.48997534179687500000e+03) (3, 6.15256896972656250000e+02) (4, -2.80156036376953125000e+02) (5, 3.39144444465637207031e+00) (6, -9.23543029785156250000e+02) (7, -1.15426293945312500000e+03) (8, 3.70295715332031250000e+01) (9, 1.73006641864776611328e+00) (0, 4.08506393432617187500e+00) (1, 1.12857565283775329590e-01) (2, -1.13635790348052978516e+00) (3, -6.08717775344848632812e+00) (4, -6.74073600769042968750e+00) (5, -2.47323393821716308594e+00) (6, 1.05438461303710937500e+02) (7, 8.47623407840728759766e-01) (8, -7.61016702651977539062e+00) (9, 6.35034179687500000000e+00) (10, 7.09308815002441406250e+00) (11, -1.70025479793548583984e+00) (12, 5.34787292480468750000e+02) (13, 1.79985117912292480469e+00) (14, 1.14821996688842773438e+01) (15, 1.28634190559387207031e+00) (16, -1.23685808181762695312e+01) (17, 3.48553705215454101562e+00) (18, -1.52308896183967590332e-01) (10, -3.99624389648437500000e+02) (11, -1.78980004787445068359e+00) (12, -9.35497894287109375000e+01) (13, 1.31213928222656250000e+03) (14, 7.27592239379882812500e+01) (15, -4.40569458007812500000e+01) (16, -4.35966205596923828125e+00) (17, -1.03984379768371582031e+00) (18, -2.06829234957695007324e-01) (19, 2.02922511100769042969e+00) (20, 2.79708318412303924561e-02) (21, -1.18009638786315917969e+00) 
