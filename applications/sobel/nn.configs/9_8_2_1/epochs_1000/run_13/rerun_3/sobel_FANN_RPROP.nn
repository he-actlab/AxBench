FANN_FLO_2.1
num_layers=4
learning_rate=0.500000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=10 9 3 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (3, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -1.60525550842285156250e+01) (1, -1.64074510335922241211e-01) (2, 3.85713964700698852539e-01) (3, -2.64849548339843750000e+01) (4, -1.00313234329223632812e+01) (5, -2.18546485900878906250e+00) (6, -1.76413571834564208984e+00) (7, -1.48004431152343750000e+03) (8, -4.62629127502441406250e+00) (9, 7.51128101348876953125e+00) (0, -4.52778747558593750000e+02) (1, -1.43038821220397949219e+00) (2, -8.14330279827117919922e-01) (3, -1.47630383300781250000e+03) (4, -4.38887625932693481445e-01) (5, -3.60277071595191955566e-02) (6, -3.02692699432373046875e+00) (7, -4.04888331890106201172e-01) (8, 1.48772001266479492188e+00) (9, 4.67141008377075195312e+00) (0, 3.91307562589645385742e-01) (1, -2.90508568286895751953e-01) (2, 4.96845293045043945312e+00) (3, -2.26208567619323730469e+00) (4, -1.35910320281982421875e+00) (5, -9.70664441585540771484e-01) (6, 7.87529826164245605469e-01) (7, -6.83072948455810546875e+00) (8, -7.25884079933166503906e-01) (9, 3.49274516105651855469e+00) (0, -1.45222521972656250000e+03) (1, -1.20761826634407043457e-01) (2, 4.40824151039123535156e-01) (3, -1.68300140380859375000e+02) (4, -2.10133099555969238281e+00) (5, -5.43648064136505126953e-01) (6, -3.59555101394653320312e+00) (7, -1.21992194652557373047e+00) (8, 3.57142591476440429688e+00) (9, 4.23262500762939453125e+00) (0, -1.60456314086914062500e+01) (1, -1.04655541479587554932e-01) (2, 4.52692240476608276367e-01) (3, -2.64090232849121093750e+01) (4, -1.01270036697387695312e+01) (5, -2.19583344459533691406e+00) (6, -1.86240422725677490234e+00) (7, -1.48004431152343750000e+03) (8, -4.46773290634155273438e+00) (9, 7.43998908996582031250e+00) (0, -1.04549634456634521484e+00) (1, -1.06091666221618652344e+00) (2, 7.74906063079833984375e+00) (3, -9.65520799160003662109e-01) (4, -3.04138922691345214844e+00) (5, -1.30068945884704589844e+00) (6, 1.13548612594604492188e+00) (7, -7.02076244354248046875e+00) (8, -3.96205306053161621094e-01) (9, 3.46789646148681640625e+00) (0, -1.03215527534484863281e+00) (1, -1.16214036941528320312e+00) (2, 6.88212823867797851562e+00) (3, -1.05249273777008056641e+00) (4, -2.67599296569824218750e+00) (5, -9.34165298938751220703e-01) (6, 5.20784437656402587891e-01) (7, -5.64914274215698242188e+00) (8, -5.46835660934448242188e-01) (9, 3.27988600730895996094e+00) (0, -1.92398414611816406250e+01) (1, -1.37163591384887695312e+01) (2, -4.74545478820800781250e+00) (3, -7.48511791229248046875e+00) (4, 1.75514769554138183594e+00) (5, -9.03646424412727355957e-02) (6, 1.19204425811767578125e+00) (7, 9.67619895935058593750e-01) (8, 1.54549674987792968750e+01) (9, 6.67919492721557617188e+00) (10, 1.50000000000000000000e+03) (11, 1.50000000000000000000e+03) (12, 8.56016349792480468750e+00) (13, 1.50000000000000000000e+03) (14, 1.50000000000000000000e+03) (15, 4.80017757415771484375e+00) (16, 4.38893365859985351562e+00) (17, 1.50000000000000000000e+03) (18, -1.17197036743164062500e+01) (10, 1.50000000000000000000e+03) (11, 1.50000000000000000000e+03) (12, 2.83034491539001464844e+00) (13, 1.50000000000000000000e+03) (14, 1.50000000000000000000e+03) (15, 1.03506593704223632812e+01) (16, 3.77084875106811523438e+00) (17, 1.50000000000000000000e+03) (18, -1.12311754226684570312e+01) (19, 1.76441335678100585938e+00) (20, 1.28964972496032714844e+00) (21, -2.26064682006835937500e+00) 
