FANN_FLO_2.1
num_layers=4
learning_rate=0.500000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=10 9 3 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (3, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -1.46061474609375000000e+03) (1, 1.23748486328125000000e+03) (2, 1.98789608478546142578e+00) (3, -1.24041296386718750000e+03) (4, -3.41668486595153808594e-01) (5, -5.21843242645263671875e+00) (6, 6.42881286621093750000e+02) (7, 2.89988918304443359375e+01) (8, -1.49653735351562500000e+03) (9, -6.94090270996093750000e+01) (0, -5.12667596340179443359e-01) (1, 2.97840738296508789062e+00) (2, 2.06884622573852539062e-01) (3, -8.15964639186859130859e-01) (4, -1.26520544290542602539e-01) (5, -6.31581783294677734375e-01) (6, -3.37851119041442871094e+00) (7, 1.15512847900390625000e+00) (8, -1.44262373447418212891e+00) (9, 5.09545505046844482422e-01) (0, 2.04811429977416992188e+00) (1, 3.00583410263061523438e+00) (2, 6.45721405744552612305e-02) (3, 9.94838327169418334961e-02) (4, -8.86518299579620361328e-01) (5, -9.64456379413604736328e-01) (6, -1.78133022785186767578e+00) (7, 6.34797632694244384766e-01) (8, -8.12964344024658203125e+00) (9, 2.59176671504974365234e-01) (0, 2.12809228897094726562e+00) (1, 2.25492787361145019531e+00) (2, 1.73758487701416015625e+01) (3, 8.07616519927978515625e+00) (4, 3.33145022392272949219e+00) (5, 6.50995612144470214844e-01) (6, 1.84826492309570312500e+02) (7, 2.00503319501876831055e-01) (8, 1.51445734500885009766e+00) (9, 8.36956024169921875000e+00) (0, 1.02708921432495117188e+01) (1, -3.30913200378417968750e+01) (2, -1.32185542583465576172e+00) (3, 6.85888624191284179688e+00) (4, -6.26055240631103515625e-01) (5, -2.11103367805480957031e+00) (6, -2.36649932861328125000e+01) (7, -2.50528240203857421875e+00) (8, -2.24938249588012695312e+00) (9, -5.22692155838012695312e+00) (0, 5.01481592655181884766e-01) (1, -2.16807961463928222656e+00) (2, -3.66510421037673950195e-01) (3, -9.87507939338684082031e-01) (4, -9.92790222167968750000e-01) (5, -1.87954747676849365234e+00) (6, 7.78129434585571289062e+00) (7, -2.24237298965454101562e+00) (8, -1.18332445621490478516e+00) (9, -4.94633579254150390625e+00) (0, 6.09928786754608154297e-01) (1, 2.48036479949951171875e+00) (2, 2.97913849353790283203e-01) (3, -6.90495669841766357422e-01) (4, 1.84629410505294799805e-02) (5, -5.98582208156585693359e-01) (6, -3.68168449401855468750e+00) (7, 1.42444038391113281250e+00) (8, -8.89990806579589843750e-01) (9, 4.26912158727645874023e-01) (0, -1.44997900390625000000e+03) (1, 3.87948913574218750000e+02) (2, 9.83039289712905883789e-02) (3, -4.11449432373046875000e+01) (4, 9.87688720226287841797e-01) (5, -3.50819993019104003906e+00) (6, 1.94744384288787841797e+00) (7, 4.41637611389160156250e+01) (8, -1.94400062561035156250e+01) (9, 8.66067707538604736328e-01) (10, 2.93011627197265625000e+02) (11, -4.61611747741699218750e+00) (12, -1.10979213714599609375e+01) (13, 3.16129016876220703125e+00) (14, 1.50000000000000000000e+03) (15, 2.68425598144531250000e+02) (16, -4.30573129653930664062e+00) (17, -1.56263160705566406250e+01) (18, 1.16800820827484130859e+00) (10, 4.57293457031250000000e+02) (11, -5.80561697483062744141e-01) (12, -9.89718079566955566406e-01) (13, -1.03014862537384033203e+00) (14, -1.50000000000000000000e+03) (15, 1.35129064941406250000e+03) (16, 1.50480639934539794922e+00) (17, 6.78373813629150390625e-01) (18, -3.90216469764709472656e-01) (19, -3.99465680122375488281e+00) (20, 8.09276401996612548828e-01) (21, 4.13257300853729248047e-01) 
