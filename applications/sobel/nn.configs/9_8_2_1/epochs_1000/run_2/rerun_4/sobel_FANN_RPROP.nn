FANN_FLO_2.1
num_layers=4
learning_rate=0.500000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=10 9 3 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (3, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 3.96076917648315429688e+00) (1, 6.07081174850463867188e+00) (2, 1.34318809509277343750e+01) (3, -4.35799217224121093750e+00) (4, 3.10731321573257446289e-01) (5, 8.34062457084655761719e-01) (6, -9.49445629119873046875e+00) (7, -1.85711021423339843750e+01) (8, 1.77735912799835205078e+00) (9, 2.92682051658630371094e-01) (0, -1.47576568603515625000e+02) (1, 1.50000000000000000000e+03) (2, 1.50000000000000000000e+03) (3, 1.50000000000000000000e+03) (4, 1.50000000000000000000e+03) (5, 1.50000000000000000000e+03) (6, 1.50000000000000000000e+03) (7, -1.38840051269531250000e+03) (8, 1.50000000000000000000e+03) (9, 1.50000000000000000000e+03) (0, 4.48954010009765625000e+00) (1, 5.83713054656982421875e+00) (2, 1.31183719635009765625e+01) (3, -4.33291196823120117188e+00) (4, 3.66961032152175903320e-01) (5, 8.50088298320770263672e-01) (6, -9.02303791046142578125e+00) (7, -1.86186027526855468750e+01) (8, 8.08190107345581054688e-01) (9, 3.98746460676193237305e-01) (0, -7.95003128051757812500e+01) (1, -1.49990527343750000000e+03) (2, -1.48804553222656250000e+03) (3, -1.14008369445800781250e+01) (4, 7.18375861644744873047e-01) (5, 1.10479915142059326172e+00) (6, 1.45582113265991210938e+01) (7, -8.15861463546752929688e-01) (8, 1.19505512714385986328e+00) (9, -3.92495632171630859375e-01) (0, 3.24121332168579101562e+00) (1, 1.50000000000000000000e+03) (2, 1.90663101196289062500e+02) (3, 4.46850372314453125000e+02) (4, 1.49999987792968750000e+03) (5, 1.49999987792968750000e+03) (6, 8.05765342712402343750e+00) (7, 1.49981225585937500000e+03) (8, 1.49633422851562500000e+03) (9, 2.10244488716125488281e+00) (0, 3.10598945617675781250e+00) (1, 1.84577596187591552734e+00) (2, 3.72241163253784179688e+00) (3, -4.93132829666137695312e+00) (4, 1.57284975051879882812e+00) (5, 6.17287039756774902344e-01) (6, -9.55252647399902343750e+00) (7, -7.01875877380371093750e+00) (8, -2.04864835739135742188e+00) (9, 4.90381097793579101562e+00) (0, -9.97046890258789062500e+01) (1, -1.49468273925781250000e+03) (2, -1.48465917968750000000e+03) (3, -1.13346862792968750000e+01) (4, 6.03349506855010986328e-01) (5, 8.22437644004821777344e-01) (6, 4.32134971618652343750e+01) (7, -6.86122179031372070312e-01) (8, 1.02217113971710205078e+00) (9, -1.13926976919174194336e-01) (0, 6.08309364318847656250e+00) (1, 3.94662928581237792969e+00) (2, 3.94186425209045410156e+00) (3, -3.26608037948608398438e+00) (4, 5.94816029071807861328e-01) (5, 4.80107128620147705078e-01) (6, -5.38893985748291015625e+00) (7, -9.50354099273681640625e+00) (8, -2.69134283065795898438e-01) (9, 1.05179116129875183105e-01) (10, 3.78758406639099121094e+00) (11, 3.81424218416213989258e-01) (12, 3.59101366996765136719e+00) (13, 4.52409172058105468750e+00) (14, -1.27810704708099365234e+00) (15, 2.34554004669189453125e+00) (16, 4.69690895080566406250e+00) (17, 3.35764773190021514893e-02) (18, -3.09569978713989257812e+00) (10, 3.60890460014343261719e+00) (11, -2.79691791534423828125e+00) (12, 3.34870648384094238281e+00) (13, 5.77797269821166992188e+00) (14, -1.62356638908386230469e+00) (15, 3.43943095207214355469e+00) (16, 4.85363626480102539062e+00) (17, 7.47263193130493164062e-01) (18, -7.94871449470520019531e-01) (19, 1.95139563083648681641e+00) (20, 3.46811115741729736328e-01) (21, -1.41506814956665039062e+00) 
