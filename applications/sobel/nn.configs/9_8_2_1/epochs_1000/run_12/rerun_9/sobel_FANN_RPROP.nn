FANN_FLO_2.1
num_layers=4
learning_rate=0.500000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=10 9 3 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (3, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -6.13289499282836914062e+00) (1, -3.86364912986755371094e+00) (2, -3.30287146568298339844e+00) (3, 3.28045308589935302734e-01) (4, 3.85368275642395019531e+00) (5, 3.32063698768615722656e+00) (6, -5.34855186939239501953e-01) (7, -7.05756127834320068359e-01) (8, 9.04403507709503173828e-01) (9, -1.09870374202728271484e+00) (0, 2.70299482345581054688e+00) (1, 4.22336399555206298828e-01) (2, -1.95773804187774658203e+00) (3, -3.75149273872375488281e+00) (4, 1.30812282562255859375e+01) (5, 1.75106573104858398438e+00) (6, -5.99133796691894531250e+01) (7, -4.52940988540649414062e+00) (8, 1.21202516555786132812e+00) (9, -4.12659597396850585938e+00) (0, -9.27970600128173828125e+00) (1, -4.75675582885742187500e+00) (2, -3.07766151428222656250e+00) (3, -1.18890523910522460938e-01) (4, 2.86942267417907714844e+00) (5, 5.38119888305664062500e+00) (6, -2.39922076463699340820e-01) (7, -5.97766637802124023438e-01) (8, 5.08208513259887695312e-01) (9, 3.53368282318115234375e-01) (0, -1.36406159400939941406e+00) (1, 6.52271807193756103516e-01) (2, 7.92960214614868164062e+00) (3, -1.35434210300445556641e-01) (4, -3.39149427413940429688e+00) (5, 3.24523973464965820312e+00) (6, -9.44612771272659301758e-02) (7, -1.04653768539428710938e+01) (8, -2.60168933868408203125e+00) (9, 1.09220564365386962891e+00) (0, -2.06769919395446777344e+00) (1, 2.41139233112335205078e-01) (2, 4.59763295948505401611e-02) (3, -6.70524358749389648438e+00) (4, 5.09239673614501953125e+00) (5, 3.67299270629882812500e+00) (6, 2.83848237991333007812e+00) (7, -4.19108295440673828125e+00) (8, 1.71501064300537109375e+00) (9, 1.08295492827892303467e-01) (0, -4.72360181808471679688e+00) (1, -8.44707340002059936523e-02) (2, 3.25158739089965820312e+00) (3, -1.31613314151763916016e-01) (4, -8.22772979736328125000e-01) (5, 5.03754949569702148438e+00) (6, -2.61035704612731933594e+00) (7, -1.30389916896820068359e+00) (8, 1.16391956806182861328e+00) (9, -1.47597765922546386719e+00) (0, 2.11152420043945312500e+01) (1, -3.69507014751434326172e-01) (2, -4.29626035690307617188e+00) (3, -2.89623999595642089844e+00) (4, 1.66504669189453125000e+00) (5, 3.55793547630310058594e+00) (6, -4.24918651580810546875e+00) (7, -4.72845029830932617188e+00) (8, 3.21791470050811767578e-01) (9, -1.10008163452148437500e+01) (0, 8.99436250329017639160e-02) (1, 1.19006514549255371094e+00) (2, 6.34705257415771484375e+00) (3, -1.38876318931579589844e+00) (4, -9.45044398307800292969e-01) (5, 6.20972251892089843750e+00) (6, -3.88255977630615234375e+00) (7, -8.49477767944335937500e+00) (8, -1.08403730392456054688e+00) (9, -6.52374982833862304688e-01) (10, -2.09033751487731933594e+00) (11, 3.29790878295898437500e+01) (12, -6.73219394683837890625e+00) (13, -8.81228327751159667969e-01) (14, 8.05921459197998046875e+00) (15, -1.12609662115573883057e-01) (16, -6.55376098632812500000e+02) (17, -1.03870401382446289062e+01) (18, 2.44332313537597656250e-01) (10, -2.88102507591247558594e+00) (11, -1.22884750366210937500e+02) (12, -6.50253343582153320312e+00) (13, -3.36865758895874023438e+00) (14, 3.11395502090454101562e+00) (15, 4.46123504638671875000e+00) (16, 7.02709341049194335938e+00) (17, -4.97647285461425781250e+00) (18, 1.99447691440582275391e-01) (19, -2.54380059242248535156e+00) (20, -2.78630495071411132812e+00) (21, 1.19824457168579101562e+00) 
