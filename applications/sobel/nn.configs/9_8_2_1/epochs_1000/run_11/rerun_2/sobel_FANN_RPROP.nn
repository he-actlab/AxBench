FANN_FLO_2.1
num_layers=4
learning_rate=0.500000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=10 9 3 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (3, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -9.94471679687500000000e+02) (1, -1.31657031250000000000e+03) (2, 4.01333526611328125000e+02) (3, 6.14174621582031250000e+02) (4, -1.38493713378906250000e+03) (5, -3.63152809143066406250e+01) (6, -1.19693870544433593750e+01) (7, 2.69812059402465820312e+00) (8, 7.85160766601562500000e+02) (9, 7.26855814456939697266e-01) (0, 5.45770168304443359375e+00) (1, 2.67217850685119628906e+00) (2, -8.42570972442626953125e+00) (3, 1.46092665195465087891e+00) (4, 1.38697052001953125000e+00) (5, -7.22065973281860351562e+00) (6, -9.61670339107513427734e-01) (7, 1.38353557586669921875e+01) (8, -1.17064723968505859375e+01) (9, 6.66645109653472900391e-01) (0, 3.00235481262207031250e+01) (1, 5.83566427230834960938e+00) (2, 6.31175231933593750000e+00) (3, -4.31094093322753906250e+01) (4, 1.99818646907806396484e+00) (5, -1.42239761352539062500e+01) (6, -6.20320653915405273438e+00) (7, 9.98233318328857421875e-01) (8, -1.10110807418823242188e+01) (9, 3.84390115737915039062e-01) (0, -4.30732297897338867188e+00) (1, 1.04365053176879882812e+01) (2, -1.84993550181388854980e-01) (3, 6.33267354965209960938e+00) (4, 5.01605606079101562500e+00) (5, -9.97227001190185546875e+00) (6, -6.86524486541748046875e+00) (7, -5.29644107818603515625e+00) (8, 2.94687319546937942505e-02) (9, 9.07575130462646484375e-01) (0, 6.69038867950439453125e+00) (1, 2.03480884432792663574e-01) (2, 1.24963425099849700928e-01) (3, 1.66799221038818359375e+01) (4, 1.31188640594482421875e+01) (5, -3.75800666809082031250e+01) (6, -1.27188224792480468750e+01) (7, 1.74392819404602050781e+00) (8, -4.82156085968017578125e+00) (9, 1.14436829090118408203e+00) (0, -2.16708847045898437500e+02) (1, 3.74546551704406738281e+00) (2, -3.15662455558776855469e+00) (3, 1.60465911865234375000e+02) (4, -3.36035013198852539062e+00) (5, 1.31893320083618164062e+01) (6, -4.61793184280395507812e+00) (7, 1.40624237060546875000e+01) (8, -1.24533767700195312500e+01) (9, 1.46186637878417968750e+00) (0, -8.98645496368408203125e+00) (1, 3.32671451568603515625e+00) (2, -5.45259046554565429688e+00) (3, 4.76953268051147460938e+00) (4, 9.98643302917480468750e+00) (5, -4.95958757400512695312e+00) (6, -4.89808130264282226562e+00) (7, -1.09661817550659179688e+00) (8, -6.46723628044128417969e-01) (9, 2.62936615943908691406e+00) (0, -1.49191687011718750000e+03) (1, -1.32779687500000000000e+03) (2, -1.74277679443359375000e+02) (3, -9.29329406738281250000e+02) (4, -9.89058303833007812500e+01) (5, -1.83350257873535156250e+01) (6, -3.15686416625976562500e+00) (7, 5.97041034698486328125e+00) (8, -9.51745849609375000000e+02) (9, 1.48256942629814147949e-01) (10, 1.01217842102050781250e+01) (11, -1.29464489746093750000e+03) (12, -6.25319702148437500000e+02) (13, 5.81680259704589843750e+01) (14, 1.48302661132812500000e+03) (15, 1.32867126464843750000e+03) (16, 4.31553344726562500000e+01) (17, 1.50000000000000000000e+03) (18, -5.31860494613647460938e+00) (10, 1.01217842102050781250e+01) (11, -6.18614746093750000000e+02) (12, -7.05530883789062500000e+02) (13, 1.03403121948242187500e+02) (14, 1.49611193847656250000e+03) (15, 1.50000000000000000000e+03) (16, 7.12435226440429687500e+01) (17, 1.50000000000000000000e+03) (18, -5.35607004165649414062e+00) (19, 8.28161776065826416016e-01) (20, 1.17420411109924316406e+00) (21, -1.14448130130767822266e+00) 
