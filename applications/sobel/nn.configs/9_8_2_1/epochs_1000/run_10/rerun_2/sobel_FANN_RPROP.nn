FANN_FLO_2.1
num_layers=4
learning_rate=0.500000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=10 9 3 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (3, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -5.05624818801879882812e+00) (1, 5.60234896838665008545e-02) (2, -1.10824708938598632812e+01) (3, 5.95349848270416259766e-01) (4, 9.00074958801269531250e-01) (5, 7.60135233402252197266e-01) (6, 6.33171975612640380859e-01) (7, -5.92627525329589843750e-02) (8, -3.53266805410385131836e-01) (9, 3.85141223669052124023e-01) (0, -1.25780403614044189453e+00) (1, 7.31523895263671875000e+00) (2, -2.32117080688476562500e+00) (3, 1.94023206830024719238e-01) (4, 2.18344736099243164062e+00) (5, 2.36083364486694335938e+00) (6, 3.44516009092330932617e-01) (7, -3.07253289222717285156e+00) (8, -7.10372030735015869141e-01) (9, -1.04977540671825408936e-01) (0, 3.37437939643859863281e+00) (1, -9.05616939067840576172e-01) (2, -1.71661338806152343750e+01) (3, 3.01859706640243530273e-01) (4, -8.64157900214195251465e-02) (5, 2.41927313804626464844e+00) (6, 1.27724647521972656250e-01) (7, 1.07571249008178710938e+01) (8, 4.71317815780639648438e+00) (9, -1.48231953382492065430e-01) (0, 2.52172684669494628906e+00) (1, -9.82822227478027343750e+00) (2, 2.31033539772033691406e+00) (3, -1.50681883096694946289e-01) (4, -2.91290760040283203125e+00) (5, -1.50691485404968261719e+00) (6, 8.76404345035552978516e-01) (7, 2.85054826736450195312e+00) (8, 2.74101829528808593750e+00) (9, 4.45839047431945800781e-01) (0, -3.44478106498718261719e+00) (1, 7.56565237045288085938e+00) (2, 3.84690189361572265625e+00) (3, -6.60844326019287109375e-01) (4, 3.24654912948608398438e+00) (5, 7.18566402792930603027e-02) (6, 3.38241100311279296875e-01) (7, -2.24280762672424316406e+00) (8, -6.34025573730468750000e-01) (9, -1.29051101207733154297e+00) (0, 1.35011649131774902344e+00) (1, -6.90391492843627929688e+00) (2, 2.82777667045593261719e+00) (3, 1.46996170282363891602e-01) (4, -2.12536311149597167969e+00) (5, -1.85607457160949707031e+00) (6, -1.36208486557006835938e+00) (7, 4.83702659606933593750e-01) (8, 3.93512392044067382812e+00) (9, -5.16592323780059814453e-01) (0, 4.12458467483520507812e+00) (1, -6.14196443557739257812e+00) (2, 1.92563934326171875000e+01) (3, -1.34231552481651306152e-01) (4, -1.93591499328613281250e+00) (5, 2.24581211805343627930e-01) (6, -4.11722145080566406250e+01) (7, 1.77661914825439453125e+01) (8, 2.59953975677490234375e+00) (9, 1.98534893989562988281e+00) (0, 9.42318022251129150391e-01) (1, -1.48432474136352539062e+01) (2, 4.19771559536457061768e-02) (3, 7.04108905792236328125e+00) (4, -2.92171454429626464844e+00) (5, -5.26396131515502929688e+00) (6, 1.61067409515380859375e+01) (7, 6.34843540191650390625e+00) (8, 2.20108580589294433594e+00) (9, 1.42184779047966003418e-01) (10, 2.75473136901855468750e+01) (11, 4.62873029708862304688e+00) (12, -2.91360688209533691406e+00) (13, -2.37036800384521484375e+01) (14, 1.10105247497558593750e+01) (15, -6.71437501907348632812e-01) (16, -3.94483923912048339844e+00) (17, -2.32870697975158691406e+00) (18, 3.95702719688415527344e-01) (10, 1.56229782104492187500e+01) (11, 3.77401304244995117188e+00) (12, -1.51144111156463623047e+00) (13, -3.31248831748962402344e+00) (14, 3.40468859672546386719e+00) (15, -4.47927093505859375000e+00) (16, -1.37358629703521728516e+00) (17, -5.08116006851196289062e+00) (18, 4.13145691156387329102e-01) (19, 1.59536457061767578125e+00) (20, 1.54117488861083984375e+00) (21, -2.25217819213867187500e+00) 
