FANN_FLO_2.1
num_layers=4
learning_rate=0.500000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=10 9 5 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (5, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -3.05275845527648925781e+00) (1, -2.80400466918945312500e+00) (2, -5.27872070312500000000e+02) (3, 3.83284568786621093750e-01) (4, -1.08097848892211914062e+01) (5, 1.05190803527832031250e+02) (6, 3.33794534206390380859e-01) (7, 3.03054779767990112305e-01) (8, -1.03275550842285156250e+02) (9, -4.49897170066833496094e-01) (0, -2.28677988052368164062e+00) (1, -4.89953279495239257812e-01) (2, 4.55955028533935546875e+00) (3, 4.35755312442779541016e-01) (4, -1.36288299560546875000e+01) (5, 2.51038169860839843750e+00) (6, 4.71382170915603637695e-01) (7, 2.47211790084838867188e+00) (8, 1.91915380954742431641e+00) (9, 1.59815692901611328125e+00) (0, -4.34340476989746093750e-01) (1, -5.65917015075683593750e-01) (2, 6.00889158248901367188e+00) (3, 7.22952857613563537598e-02) (4, -1.57084064483642578125e+01) (5, 3.05878043174743652344e+00) (6, 5.70770406723022460938e+00) (7, -1.05240488052368164062e+00) (8, 3.25880758464336395264e-02) (9, 5.00124454498291015625e-01) (0, -7.14706230163574218750e+00) (1, -3.32425498962402343750e+00) (2, -8.57643188476562500000e+02) (3, 4.19356375932693481445e-01) (4, -1.01146497726440429688e+01) (5, 3.46105995178222656250e+01) (6, 4.10547226667404174805e-01) (7, 5.53856909275054931641e-01) (8, -2.19595966339111328125e+01) (9, -3.75770783424377441406e+00) (0, -3.40180230140686035156e+00) (1, 2.44643285870552062988e-01) (2, 5.62220430374145507812e+00) (3, 7.92814567685127258301e-02) (4, -1.93212661743164062500e+01) (5, 1.62077748775482177734e+00) (6, 1.58075821399688720703e+00) (7, 4.85755968093872070312e+00) (8, -2.88585972785949707031e+00) (9, 3.26242637634277343750e+00) (0, -3.47761955261230468750e+01) (1, -1.85749320983886718750e+01) (2, -5.31794189453125000000e+02) (3, 3.04018855094909667969e-01) (4, -9.45052433013916015625e+00) (5, -1.50000000000000000000e+03) (6, 4.58766162395477294922e-01) (7, -1.50000000000000000000e+03) (8, -2.76705875396728515625e+01) (9, 1.88771970570087432861e-02) (0, 1.23705840110778808594e+00) (1, -3.97506982088088989258e-01) (2, -1.31433510780334472656e+00) (3, 2.02587699890136718750e+00) (4, -9.12996768951416015625e+00) (5, 3.51031994819641113281e+00) (6, 1.40023219585418701172e+00) (7, 7.01153802871704101562e+00) (8, -5.40064930915832519531e-01) (9, 3.65356564521789550781e-01) (0, 7.84761071205139160156e-01) (1, -3.97894692420959472656e+00) (2, 1.40801181793212890625e+01) (3, 2.26403176784515380859e-01) (4, -1.58417320251464843750e+01) (5, 3.27653908729553222656e+00) (6, 9.10406112670898437500e+00) (7, 1.48478102684020996094e+00) (8, -2.31637239456176757812e+00) (9, -1.50293457508087158203e+00) (10, -2.80053497314453125000e+02) (11, 5.64485788345336914062e-01) (12, -7.65479326248168945312e+00) (13, 1.00466299057006835938e+01) (14, 3.88910102844238281250e+00) (15, 1.50000000000000000000e+03) (16, 7.22832500934600830078e-01) (17, 2.35185003280639648438e+00) (18, -6.05639338493347167969e-01) (10, -7.30368408203125000000e+02) (11, 1.06240997314453125000e+01) (12, -1.46516271972656250000e+03) (13, 3.70123672485351562500e+01) (14, 1.82535232543945312500e+02) (15, 1.50000000000000000000e+03) (16, 1.24438133239746093750e+01) (17, -2.09048248291015625000e+02) (18, 1.45594131946563720703e+00) (10, 1.50000000000000000000e+03) (11, 4.30396652221679687500e+00) (12, 2.19802474975585937500e+00) (13, 2.61890525817871093750e+01) (14, -6.93775272369384765625e+00) (15, 1.50000000000000000000e+03) (16, -3.35023641586303710938e-01) (17, -6.29108190536499023438e+00) (18, 3.59865188598632812500e+00) (10, -1.50000000000000000000e+03) (11, -2.83136546611785888672e-01) (12, -5.37758684158325195312e+00) (13, -4.91410636901855468750e+01) (14, 3.75454616546630859375e+00) (15, 1.50000000000000000000e+03) (16, 6.85162544250488281250e-01) (17, 2.08032560348510742188e+00) (18, -7.51539707183837890625e-01) (19, -2.74100780487060546875e+00) (20, -7.16511607170104980469e-01) (21, 2.80946922302246093750e+00) (22, -9.07121300697326660156e-01) (23, 7.66811370849609375000e-02) 
