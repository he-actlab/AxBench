FANN_FLO_2.1
num_layers=4
learning_rate=0.500000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=10 9 5 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (5, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 2.67762827873229980469e+00) (1, 1.44467147827148437500e+02) (2, 1.04536592960357666016e+00) (3, 1.47862493991851806641e+00) (4, 7.56973419189453125000e+01) (5, 1.08759813308715820312e+01) (6, -8.31729965209960937500e+01) (7, -8.21110486984252929688e-01) (8, 5.34125785827636718750e+01) (9, -1.11528884887695312500e+02) (0, 5.90789461135864257812e+00) (1, 1.56525497436523437500e+01) (2, -7.73245021700859069824e-02) (3, 5.41013145446777343750e+01) (4, 7.27226305007934570312e+00) (5, -6.04889183044433593750e+01) (6, -5.22961974143981933594e-01) (7, -1.28732407093048095703e+00) (8, -3.40158963203430175781e+00) (9, -2.29463398456573486328e-01) (0, 6.44709646701812744141e-01) (1, 4.57561225891113281250e+01) (2, -8.58941197395324707031e-01) (3, 4.60230293273925781250e+01) (4, 7.80064344406127929688e+00) (5, -4.24019126892089843750e+01) (6, -3.28860321044921875000e+01) (7, 1.09334051609039306641e-01) (8, -6.11224699020385742188e+00) (9, -8.18560004234313964844e-01) (0, -2.71598815917968750000e+02) (1, -1.69962722778320312500e+02) (2, -4.11140930175781250000e+02) (3, 4.68764190673828125000e+01) (4, -1.47002612304687500000e+03) (5, -6.11188537597656250000e+02) (6, 1.48833056640625000000e+03) (7, 2.25368061065673828125e+01) (8, -8.24183227539062500000e+02) (9, 6.22986841201782226562e+00) (0, 3.85296058654785156250e+00) (1, -2.02480888366699218750e+00) (2, 2.99979376792907714844e+00) (3, -7.91368198394775390625e+00) (4, -3.86453485488891601562e+00) (5, -1.40584974288940429688e+01) (6, -2.67687606811523437500e+01) (7, -8.11481189727783203125e+00) (8, -3.62908363342285156250e+01) (9, -5.26967334747314453125e+00) (0, 1.68938064575195312500e+00) (1, 7.41098632812500000000e+01) (2, 7.65191972255706787109e-01) (3, 5.41612863540649414062e-01) (4, 1.79676043987274169922e+00) (5, 9.95633773803710937500e+01) (6, -1.32735549926757812500e+02) (7, -1.52254089713096618652e-01) (8, 1.92212867736816406250e+01) (9, -1.38708543777465820312e+01) (0, 1.24557733535766601562e+01) (1, 3.28587875366210937500e+01) (2, 3.06468486785888671875e+00) (3, 1.62818939208984375000e+02) (4, 4.89194440841674804688e+00) (5, 6.47617111206054687500e+01) (6, -2.73087097167968750000e+02) (7, -1.45379817485809326172e+00) (8, 1.42346944808959960938e+01) (9, -5.73038673400878906250e+00) (0, 2.70498132705688476562e+00) (1, 2.54312553405761718750e+01) (2, -2.57781416177749633789e-01) (3, 1.19009485244750976562e+01) (4, 1.21425590515136718750e+01) (5, 6.06802225112915039062e+00) (6, -2.04937393188476562500e+02) (7, 4.32364511489868164062e+00) (8, 2.54215049743652343750e+01) (9, -7.36525440216064453125e+00) (10, 2.25679336547851562500e+02) (11, -4.67065338134765625000e+02) (12, -1.90012226104736328125e+01) (13, 7.69403625488281250000e+02) (14, 1.50000000000000000000e+03) (15, 7.41707382202148437500e+01) (16, -3.54683837890625000000e+02) (17, 6.86452575683593750000e+02) (18, 3.20020629882812500000e+02) (10, -7.04707275390625000000e+02) (11, -4.86489196777343750000e+02) (12, -4.91380615234375000000e+02) (13, 5.63180297851562500000e+02) (14, 9.73503173828125000000e+02) (15, -5.47688598632812500000e+02) (16, -4.33227294921875000000e+02) (17, -4.33871063232421875000e+02) (18, 7.20957994461059570312e+00) (10, 1.50000000000000000000e+03) (11, 8.45353637695312500000e+02) (12, 2.56330596923828125000e+02) (13, -3.07644165039062500000e+02) (14, -5.71878906250000000000e+02) (15, 1.00422308349609375000e+03) (16, 8.53761962890625000000e+02) (17, 4.04505584716796875000e+02) (18, -1.30049085617065429688e+01) (10, -7.04707275390625000000e+02) (11, -4.86489196777343750000e+02) (12, -4.91380615234375000000e+02) (13, 5.63180297851562500000e+02) (14, 9.73503173828125000000e+02) (15, -5.47688598632812500000e+02) (16, -4.33227294921875000000e+02) (17, -4.33871063232421875000e+02) (18, 4.24286693334579467773e-01) (19, 9.45392310619354248047e-01) (20, 1.29474658203125000000e+03) (21, -3.99457305669784545898e-01) (22, 1.29474658203125000000e+03) (23, 3.30895811319351196289e-01) 
