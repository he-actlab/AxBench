FANN_FLO_2.1
num_layers=4
learning_rate=0.500000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=10 9 9 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (9, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 1.15134611129760742188e+01) (1, 6.01370716094970703125e+00) (2, 5.94430017471313476562e+00) (3, 3.99430632591247558594e-01) (4, 7.22315073013305664062e+00) (5, 8.98132622241973876953e-01) (6, 8.34910511970520019531e-01) (7, -8.37094497680664062500e+00) (8, -7.83852148056030273438e+00) (9, 8.19630861282348632812e-01) (0, 7.17987012863159179688e+00) (1, 2.69513339996337890625e+01) (2, 8.68202209472656250000e+00) (3, -2.71324545145034790039e-01) (4, -1.59012746810913085938e+00) (5, 4.34149703979492187500e+01) (6, -8.51581001281738281250e+00) (7, -9.30205523967742919922e-01) (8, -4.54277038574218750000e+01) (9, 7.41893053054809570312e-01) (0, 1.49944433593750000000e+03) (1, 1.49961340332031250000e+03) (2, 1.50000000000000000000e+03) (3, 1.00566200256347656250e+02) (4, 7.28585876464843750000e+02) (5, 1.47855102539062500000e+03) (6, 1.29397460937500000000e+03) (7, 1.50000000000000000000e+03) (8, 1.50000000000000000000e+03) (9, 4.92642116546630859375e+00) (0, 1.49944433593750000000e+03) (1, 1.49961340332031250000e+03) (2, 1.50000000000000000000e+03) (3, 1.49808190917968750000e+03) (4, 9.56545532226562500000e+02) (5, 1.47855102539062500000e+03) (6, 1.25879138183593750000e+03) (7, 1.50000000000000000000e+03) (8, 1.50000000000000000000e+03) (9, 7.63554443359375000000e+02) (0, -1.47698569297790527344e+00) (1, -9.46873283386230468750e+00) (2, -3.47505998611450195312e+00) (3, 1.54357254505157470703e+00) (4, 4.73845273256301879883e-01) (5, -3.27069401741027832031e+00) (6, 6.00309610366821289062e+00) (7, 5.57325839996337890625e+00) (8, 1.01976566314697265625e+01) (9, -1.66519081592559814453e+00) (0, -5.01525819301605224609e-01) (1, -3.92389869689941406250e+01) (2, -2.82774353027343750000e+00) (3, 2.33935177326202392578e-01) (4, 1.75088390707969665527e-01) (5, -8.59487891197204589844e-01) (6, 3.22468042373657226562e+00) (7, -3.15784788131713867188e+00) (8, 2.16870555877685546875e+01) (9, -2.35496687889099121094e+00) (0, 4.26704359054565429688e+00) (1, -1.82386169433593750000e+01) (2, -1.50000000000000000000e+03) (3, 1.72250485420227050781e+00) (4, 9.38060760498046875000e+00) (5, -1.45648400878906250000e+03) (6, -2.61796832084655761719e-01) (7, 7.72001683712005615234e-01) (8, -1.50000000000000000000e+03) (9, 1.02675318717956542969e+00) (0, 4.82878417968750000000e+02) (1, 1.49961340332031250000e+03) (2, 1.50000000000000000000e+03) (3, 2.03953361511230468750e+01) (4, 5.30906295776367187500e+01) (5, 1.47855102539062500000e+03) (6, 8.61979431152343750000e+02) (7, 1.50000000000000000000e+03) (8, 1.50000000000000000000e+03) (9, 2.20848250389099121094e+00) (10, 3.71546298265457153320e-01) (11, -6.31083190917968750000e+02) (12, -1.89250411987304687500e+01) (13, -3.53342080116271972656e+00) (14, -1.50000000000000000000e+03) (15, -2.01946914672851562500e+02) (16, 2.74156684875488281250e+01) (17, -1.89110069274902343750e+01) (18, -4.97269010543823242188e+00) (10, 6.97602005004882812500e+01) (11, -1.50000000000000000000e+03) (12, -4.95615417480468750000e+02) (13, -1.34877648925781250000e+03) (14, -1.50000000000000000000e+03) (15, 2.70734436035156250000e+02) (16, 1.37263031005859375000e+01) (17, -3.69947082519531250000e+02) (18, -2.04553161621093750000e+02) (10, 1.83714923858642578125e+01) (11, 4.55709533691406250000e+01) (12, -1.55454711914062500000e+01) (13, -9.42466616630554199219e-01) (14, 1.53809404373168945312e+01) (15, -1.45832929611206054688e+01) (16, 8.52998924255371093750e+00) (17, -4.91354703903198242188e+00) (18, -2.13641977310180664062e+00) (10, 5.79865980148315429688e+00) (11, 5.32476091384887695312e+00) (12, -1.75013446807861328125e+00) (13, -1.04106795787811279297e+00) (14, -1.17954244613647460938e+01) (15, 6.68176651000976562500e+00) (16, 2.97790489196777343750e+01) (17, -1.27344191074371337891e-01) (18, 6.67075395584106445312e-01) (10, 5.05954399108886718750e+01) (11, -1.49869458007812500000e+03) (12, -1.38345336914062500000e+01) (13, -3.46546058654785156250e+01) (14, -5.81701538085937500000e+02) (15, 6.47711669921875000000e+02) (16, -6.71278953552246093750e+00) (17, -1.23004808425903320312e+01) (18, -3.27899193763732910156e+00) (10, 2.42648696899414062500e+00) (11, -1.08196723937988281250e+02) (12, 1.09971880912780761719e-01) (13, -3.10113310813903808594e+00) (14, -2.22703647613525390625e+01) (15, 2.84455814361572265625e+01) (16, 1.38728393554687500000e+02) (17, 2.40513682365417480469e-01) (18, 2.31172248721122741699e-01) (10, 9.70566463470458984375e+00) (11, -1.46269824218750000000e+03) (12, -2.21686077117919921875e+01) (13, -8.20964126586914062500e+01) (14, -6.17110534667968750000e+02) (15, 7.87570495605468750000e+02) (16, 1.00599508285522460938e+01) (17, -1.77927494049072265625e+01) (18, -4.79482316970825195312e+00) (10, 4.08635759353637695312e+00) (11, -2.96827911376953125000e+02) (12, -1.44973504543304443359e+00) (13, -5.16334819793701171875e+00) (14, -5.12629699707031250000e+01) (15, 6.57750854492187500000e+01) (16, 1.02834720611572265625e+01) (17, -1.17295980453491210938e+00) (18, -3.69379878044128417969e-01) (19, 1.50000000000000000000e+03) (20, 1.50000000000000000000e+03) (21, -2.62067079544067382812e+00) (22, 4.93591833114624023438e+00) (23, 4.18144822120666503906e-01) (24, 4.50103372335433959961e-01) (25, 9.16072785854339599609e-01) (26, 5.81145346164703369141e-01) (27, -1.46898972988128662109e+00) 
