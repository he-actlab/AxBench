FANN_FLO_2.1
num_layers=4
learning_rate=0.500000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=10 9 9 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (9, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 1.49949780273437500000e+03) (1, 1.50000000000000000000e+03) (2, 1.50000000000000000000e+03) (3, 1.49897827148437500000e+03) (4, 1.49989331054687500000e+03) (5, 1.50000000000000000000e+03) (6, 1.49367712402343750000e+03) (7, 1.49941772460937500000e+03) (8, 1.50000000000000000000e+03) (9, 1.46086370944976806641e+00) (0, 1.49949780273437500000e+03) (1, 1.50000000000000000000e+03) (2, 1.50000000000000000000e+03) (3, 1.49897827148437500000e+03) (4, 1.49989331054687500000e+03) (5, 1.50000000000000000000e+03) (6, 1.49367712402343750000e+03) (7, 1.49941772460937500000e+03) (8, 1.50000000000000000000e+03) (9, 1.59103832244873046875e+01) (0, 1.49949780273437500000e+03) (1, 1.50000000000000000000e+03) (2, 1.50000000000000000000e+03) (3, 1.49897827148437500000e+03) (4, 1.49989331054687500000e+03) (5, 1.50000000000000000000e+03) (6, 1.49367712402343750000e+03) (7, 1.49941772460937500000e+03) (8, 1.50000000000000000000e+03) (9, 3.44535095214843750000e+02) (0, -2.96957731246948242188e+00) (1, -8.97071266174316406250e+00) (2, 1.87520790100097656250e+00) (3, 4.20076942443847656250e+00) (4, 4.87325251102447509766e-01) (5, -1.80040478706359863281e-01) (6, 2.01638565063476562500e+01) (7, 4.62583694458007812500e+01) (8, -2.10875701904296875000e+01) (9, 3.26343393325805664062e+00) (0, -9.88080024719238281250e-01) (1, -2.86619281768798828125e+01) (2, -1.79036796092987060547e+00) (3, 5.21371459960937500000e+00) (4, 1.36914038658142089844e+00) (5, -4.45615959167480468750e+00) (6, 1.76130580902099609375e+01) (7, 7.33153057098388671875e+00) (8, 2.04410781860351562500e+01) (9, -6.69936275482177734375e+00) (0, 1.49949780273437500000e+03) (1, 1.50000000000000000000e+03) (2, 1.50000000000000000000e+03) (3, 1.49897827148437500000e+03) (4, 1.49989331054687500000e+03) (5, 1.50000000000000000000e+03) (6, 1.49367712402343750000e+03) (7, 1.49941772460937500000e+03) (8, 1.50000000000000000000e+03) (9, 2.23392105102539062500e+00) (0, 1.49949780273437500000e+03) (1, 1.50000000000000000000e+03) (2, 1.50000000000000000000e+03) (3, 1.49897827148437500000e+03) (4, 1.49989331054687500000e+03) (5, 1.50000000000000000000e+03) (6, 1.49367712402343750000e+03) (7, 1.49941772460937500000e+03) (8, 1.50000000000000000000e+03) (9, 9.02439475059509277344e-01) (0, 5.05206441879272460938e+00) (1, 1.49099731445312500000e+03) (2, 1.46424926757812500000e+02) (3, 2.17534348368644714355e-01) (4, -1.78901696205139160156e+00) (5, -6.29822254180908203125e+00) (6, -5.29375572204589843750e+01) (7, -1.59011375904083251953e+00) (8, -1.32638412475585937500e+02) (9, -1.63165710449218750000e+02) (10, -1.49999682617187500000e+03) (11, -5.28870239257812500000e+01) (12, -9.02085449218750000000e+02) (13, -1.48257934570312500000e+03) (14, -1.35514733886718750000e+03) (15, -1.50000000000000000000e+03) (16, -1.50000000000000000000e+03) (17, -1.50000000000000000000e+03) (18, -2.32428588867187500000e+01) (10, 3.79484701156616210938e+00) (11, -3.90904903411865234375e-01) (12, -4.88994002342224121094e-01) (13, 4.27273750305175781250e+00) (14, -2.88620223999023437500e+01) (15, -9.53616321086883544922e-01) (16, -8.61075699329376220703e-01) (17, -4.11103963851928710938e+00) (18, -4.77297425270080566406e-01) (10, 2.71499848365783691406e+00) (11, 1.84911513328552246094e+00) (12, 5.86450636386871337891e-01) (13, 1.38216108083724975586e-01) (14, -4.37053728103637695312e+00) (15, -4.83765870332717895508e-01) (16, -3.47169548273086547852e-01) (17, -4.09716176986694335938e+00) (18, 1.70558023452758789062e+00) (10, 1.50000000000000000000e+03) (11, 6.50755500793457031250e+00) (12, 1.47469177246093750000e+02) (13, 1.50000000000000000000e+03) (14, 1.46190490722656250000e+03) (15, 1.50000000000000000000e+03) (16, 1.50000000000000000000e+03) (17, 1.50000000000000000000e+03) (18, 6.52303647994995117188e+00) (10, -1.49999682617187500000e+03) (11, -1.38314208984375000000e+01) (12, -4.63700592041015625000e+02) (13, -1.48257934570312500000e+03) (14, -1.35514733886718750000e+03) (15, -1.50000000000000000000e+03) (16, -1.50000000000000000000e+03) (17, -1.50000000000000000000e+03) (18, -6.44902324676513671875e+00) (10, -1.49999682617187500000e+03) (11, -1.38117771148681640625e+01) (12, -4.63713897705078125000e+02) (13, -1.48257934570312500000e+03) (14, -1.35514733886718750000e+03) (15, -1.50000000000000000000e+03) (16, -1.50000000000000000000e+03) (17, -1.50000000000000000000e+03) (18, -6.54509639739990234375e+00) (10, -1.49999682617187500000e+03) (11, -1.37941513061523437500e+01) (12, -4.63755584716796875000e+02) (13, -1.48257934570312500000e+03) (14, -1.35514733886718750000e+03) (15, -1.50000000000000000000e+03) (16, -1.50000000000000000000e+03) (17, -1.50000000000000000000e+03) (18, -6.41607379913330078125e+00) (10, -1.49981530761718750000e+03) (11, -2.36850547790527343750e+01) (12, -2.39269027709960937500e+01) (13, -1.49836376953125000000e+03) (14, -1.35514733886718750000e+03) (15, -1.07010612487792968750e+01) (16, -5.08572864532470703125e+00) (17, -1.50000000000000000000e+03) (18, -5.56769447326660156250e+01) (19, 1.50000000000000000000e+03) (20, -2.23250582814216613770e-01) (21, 4.22811269760131835938e+00) (22, -9.37558829784393310547e-01) (23, 1.50000000000000000000e+03) (24, 1.50000000000000000000e+03) (25, 1.50000000000000000000e+03) (26, 1.50000000000000000000e+03) (27, -1.81325900554656982422e+00) 
