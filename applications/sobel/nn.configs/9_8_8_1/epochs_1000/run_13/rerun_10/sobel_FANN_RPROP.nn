FANN_FLO_2.1
num_layers=4
learning_rate=0.500000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=10 9 9 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (10, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (9, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 9.40195846557617187500e+00) (1, 3.13624856062233448029e-03) (2, 1.02005660533905029297e+00) (3, -2.74196207523345947266e-01) (4, -3.06973814964294433594e+00) (5, -2.90281944274902343750e+01) (6, 9.41002941131591796875e+00) (7, 4.49474543333053588867e-01) (8, -1.13551206886768341064e-01) (9, 4.05534446239471435547e-01) (0, 5.28780221939086914062e+00) (1, -4.89837799072265625000e+02) (2, 2.99765610694885253906e+00) (3, 1.53370084762573242188e+01) (4, 1.13394432067871093750e+01) (5, 6.63919601440429687500e+01) (6, 5.12535572052001953125e+00) (7, 1.54476364135742187500e+02) (8, -6.60748474121093750000e+02) (9, 7.44205993652343750000e+02) (0, 2.78875827789306640625e+00) (1, 3.54832077026367187500e+01) (2, -1.47287273406982421875e+00) (3, 2.50047950744628906250e+01) (4, 2.37837886810302734375e+01) (5, 6.18150024414062500000e+01) (6, 2.75750684738159179688e+00) (7, 3.71369514465332031250e+01) (8, -4.69550476074218750000e+02) (9, 1.64283355712890625000e+02) (0, 2.23286800384521484375e+01) (1, 1.33527099609375000000e+02) (2, 1.87955978393554687500e+02) (3, 3.54765295982360839844e-01) (4, -3.02219778299331665039e-01) (5, 1.70744860172271728516e+00) (6, 1.33922592163085937500e+02) (7, 3.22196929931640625000e+02) (8, -2.21686798095703125000e+02) (9, 4.35899436473846435547e-01) (0, 4.27039794921875000000e+01) (1, 4.09319669008255004883e-01) (2, -3.99405956268310546875e+00) (3, 5.86908459663391113281e-01) (4, -3.25014495849609375000e+00) (5, -6.72836589813232421875e+00) (6, 6.01764202117919921875e+00) (7, 1.27934026718139648438e+01) (8, 1.43848031759262084961e-01) (9, -1.74015750885009765625e+01) (0, 3.61436920166015625000e+01) (1, -1.42059147357940673828e-01) (2, -3.42532396316528320312e+00) (3, 4.21854764223098754883e-01) (4, -3.10636353492736816406e+00) (5, -5.54131841659545898438e+00) (6, 3.49559426307678222656e+00) (7, 9.24796390533447265625e+00) (8, -1.80030024051666259766e+00) (9, 1.11125669479370117188e+01) (0, 7.59771919250488281250e+00) (1, -3.72739410400390625000e+02) (2, 8.31796836853027343750e+00) (3, -3.29695910215377807617e-01) (4, 3.23613762855529785156e+00) (5, 4.29574728012084960938e-01) (6, 2.17831778526306152344e+00) (7, 1.18802318572998046875e+01) (8, -6.60871154785156250000e+02) (9, 9.65804626464843750000e+02) (0, 1.28291473388671875000e+01) (1, 2.62545824050903320312e-01) (2, 2.69046038389205932617e-01) (3, -1.69717562198638916016e+00) (4, -5.19869613647460937500e+00) (5, -2.51634445190429687500e+01) (6, 1.94133746623992919922e+00) (7, 5.16005849838256835938e+00) (8, -5.91870695352554321289e-02) (9, 1.19994707405567169189e-01) (10, -1.48620007324218750000e+03) (11, 1.84449844360351562500e+01) (12, 5.27200775146484375000e+01) (13, 3.65011096000671386719e+00) (14, 9.22186203002929687500e+01) (15, 8.82166028022766113281e-01) (16, 1.86681594848632812500e+01) (17, -9.67393920898437500000e+02) (18, 5.63622057437896728516e-01) (10, -1.14977526855468750000e+03) (11, 2.65099658966064453125e+01) (12, 2.44670867919921875000e+02) (13, 2.00475406646728515625e+00) (14, 3.91787796020507812500e+01) (15, 1.30131435394287109375e+00) (16, 1.22733392715454101562e+01) (17, -3.53810852050781250000e+02) (18, 4.59106504917144775391e-01) (10, 6.74188354492187500000e+02) (11, -1.24584293365478515625e+00) (12, -1.20367109775543212891e+00) (13, -2.74369686841964721680e-01) (14, 3.10890884399414062500e+01) (15, -8.45815086364746093750e+00) (16, -1.44961714744567871094e+00) (17, -9.17795654296875000000e+02) (18, -5.66484153270721435547e-01) (10, 1.50000000000000000000e+03) (11, 1.89271330833435058594e-01) (12, -3.40558505058288574219e+00) (13, 8.04915428161621093750e+00) (14, -2.70961547851562500000e+02) (15, 5.29006919860839843750e+01) (16, -3.05132597684860229492e-01) (17, 1.50000000000000000000e+03) (18, 3.34762930870056152344e+00) (10, -1.50000000000000000000e+03) (11, -4.33654218912124633789e-01) (12, 2.62854886054992675781e+00) (13, -4.23354297876358032227e-01) (14, -1.20520959472656250000e+03) (15, -1.32884912109375000000e+03) (16, -1.21981948614120483398e-01) (17, -1.50000000000000000000e+03) (18, -1.65639743208885192871e-01) (10, -1.50000000000000000000e+03) (11, -4.11952972412109375000e-01) (12, 2.62033700942993164062e+00) (13, -1.24653625488281250000e+00) (14, -1.25520959472656250000e+03) (15, -1.42853796386718750000e+03) (16, -2.91788995265960693359e-01) (17, -1.50000000000000000000e+03) (18, -1.04746252298355102539e-01) (10, -1.50000000000000000000e+03) (11, -4.15021598339080810547e-01) (12, 2.66868543624877929688e+00) (13, -1.30483984947204589844e+00) (14, 2.37961608886718750000e+02) (15, -1.42565063476562500000e+03) (16, -2.99988955259323120117e-01) (17, -1.50000000000000000000e+03) (18, -2.53993153572082519531e-01) (10, -1.50000000000000000000e+03) (11, -3.40742230415344238281e-01) (12, 2.47694754600524902344e+00) (13, -9.41638827323913574219e-01) (14, 2.37961608886718750000e+02) (15, -1.42565063476562500000e+03) (16, -8.89483764767646789551e-02) (17, -1.50000000000000000000e+03) (18, -1.99043169617652893066e-01) (19, 8.07512044906616210938e-01) (20, 9.49089229106903076172e-01) (21, -7.52586305141448974609e-01) (22, -1.15919053554534912109e-01) (23, 1.50000000000000000000e+03) (24, 1.50000000000000000000e+03) (25, 1.50000000000000000000e+03) (26, 1.50000000000000000000e+03) (27, -7.73739099502563476562e-01) 
